{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG8fbI5MlJ+/dj98dHbYS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/210_vectores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectores\n",
        "\n",
        "## 1. Fundamentos\n",
        "\n",
        "### Concepto de Vector\n",
        "\n",
        "- Un vector es una cantidad matemática que posee tanto magnitud (tamaño) como dirección.\n",
        "- A diferencia de los escalares (que solo tienen magnitud), los vectores son fundamentales para representar cantidades direccionales en matemáticas y sus aplicaciones.\n",
        "- En el contexto de la Inteligencia Artificial, los vectores son estructuras de datos fundamentales que permiten representar características, atributos o propiedades de manera ordenada y matemáticamente manipulable.\n",
        "- A efectos prácticos, podemos ver un vector como una lista ordenada de números.\n",
        "- Podemos hacer operaciones de traslación (suma) y de escala (multiplicación).\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_1.svg?raw=1\" alt=\"vector en una direccion\" width=\"320\"/>\n",
        "\n",
        "### Componentes de un Vector\n",
        "\n",
        "Un vector se puede representar mediante sus componentes, que son las proyecciones del vector sobre los ejes coordenados:\n",
        "\n",
        "- En 2D: un vector $\\vec{v}$ se representa como un par ordenado $(v_x, v_y)$\n",
        "- En 3D: un vector $\\vec{v}$ se representa como una terna ordenada $(v_x, v_y, v_z)$\n",
        "- En IA: un vector $\\vec{v}$ puede tener $n$ componentes $(v_1, v_2, ..., v_n)$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_11.gif?raw=1\" alt=\"componentes de un vector\" width=\"320\"/>\n",
        "\n",
        "Cada componente representa una magnitud en la dirección de un eje coordenado particular.\n",
        "\n",
        "**Ejemplos:**\n",
        "- Vector en 2D: $\\vec{v} = (4, 5)$ tiene componente $x = 4$ y componente $y = 5$\n",
        "- Vector en 3D: $\\vec{w} = (1, -2, 5)$ tiene componente $x = 1$, componente $y = -2$ y componente $z = 5$\n",
        "- Vector de características: $\\vec{f} = (8, \\,1.5,\\, -2.1,\\, 0.4)$ podría representar diferentes atributos de un dato\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_2.svg?raw=1\" alt=\"modulo direccion sentido\" width=\"320\"/>\n",
        "\n",
        "### Representación Gráfica\n",
        "\n",
        "#### En 2D\n",
        "Los vectores bidimensionales se representan en un plano cartesiano:\n",
        "- El origen del vector se ubica típicamente en $(0,0)$\n",
        "- La flecha indica la dirección y magnitud\n",
        "- Las componentes $(v_x, v_y)$ determinan que el vector comienza en el punto $(0,0)$ y finalizan en el punto $(v_x, v_y)$ del plano.\n",
        "- Ejemplo, si el vector es el $(3, 2)$, este vector se representa mediante una flecha que parte del $(0, 0)$ y termina en el punto $(3, 2)$ del plano cartesiano.\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_3.svg?raw=1\" alt=\"componentes de un vector 2D\" width=\"320\"/>\n",
        "\n",
        "#### En 3D\n",
        "Los vectores tridimensionales se representan en un espacio cartesiano:\n",
        "- El origen suele ubicarse en $(0,0,0)$\n",
        "- Las componentes $(v_x, v_y, v_z)$ determinan el punto final\n",
        "- La visualización requiere perspectiva para mostrar la profundidad\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_4.webp?raw=1\" alt=\"punto en el espacio\" width=\"320\"/>\n",
        "\n",
        "### Aplicaciones en IA\n",
        "\n",
        "Los vectores son fundamentales en IA por varias razones:\n",
        "\n",
        "1. **Representación de Datos**\n",
        "   - Cada dato se puede representar como un vector de características\n",
        "   - Ejemplo: Una imagen de 28×28 píxeles se convierte en un vector de 784 componentes\n",
        "\n",
        "2. **Embeddings**\n",
        "   - Las palabras se pueden representar como vectores en un espacio semántico\n",
        "   - Ejemplo: La palabra \"gato\" podría representarse como $\\vec{v} = (0.2, -0.5, 0.8, ...)$\n",
        "\n",
        "3. **Redes Neuronales**\n",
        "   - Las entradas y salidas son vectores\n",
        "   - Los pesos de las conexiones se almacenan como vectores\n",
        "   - Los gradientes durante el entrenamiento son vectores\n",
        "\n",
        "4. **Procesamiento de Datos**\n",
        "   - La normalización se aplica a vectores de características\n",
        "   - Las distancias entre vectores miden similitud entre datos\n",
        "   - La reducción de dimensionalidad opera sobre vectores\n",
        "\n",
        "Este concepto fundamental de vectores sirve como base para entender operaciones más complejas en machine learning y deep learning, donde los datos multidimensionales son la norma.\n",
        "\n",
        "Los vectores permiten:\n",
        "- Representar datos de manera uniforme\n",
        "- Aplicar operaciones matemáticas consistentes\n",
        "- Medir similitudes y diferencias entre datos\n",
        "- Transformar y manipular información de manera sistemática"
      ],
      "metadata": {
        "id": "KfhpPbPgCPBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Operaciones Básicas\n",
        "\n",
        "## Suma y Resta de Vectores\n",
        "\n",
        "Las operaciones de suma y resta de vectores son fundamentales en IA, especialmente en el procesamiento de datos y en el entrenamiento de modelos.\n",
        "\n",
        "### Método Algebraico\n",
        "\n",
        "#### Suma de Vectores\n",
        "\n",
        "La suma de dos vectores se realiza componente a componente:\n",
        "\n",
        "Si $\\vec{u} = (u_1, u_2, ..., u_n)$ y $\\vec{v} = (v_1, v_2, ..., v_n)$, entonces:\n",
        "\n",
        "$$\\vec{u} + \\vec{v} = (u_1 + v_1, u_2 + v_2, ..., u_n + v_n)$$\n",
        "\n",
        "**Ejemplo:**\n",
        "- $\\vec{u} = (3, 4)$ y $\\vec{v} = (1, 2)$\n",
        "- $\\vec{u} + \\vec{v} = (3+1, 4+2) = (4, 6)$"
      ],
      "metadata": {
        "id": "9fSMZi2GXdDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sumando los elementos de una lista de listas\n",
        "x = [3, 4]\n",
        "y = [1, 2]\n",
        "\n",
        "z = [u + v for u, v in zip(x, y)]   # sumando componente a componente\n",
        "\n",
        "print(f\"{x} + {y} = {z}\")"
      ],
      "metadata": {
        "outputId": "c4d4a345-2a3d-420f-cb39-01ec12c675e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1r44S_tXdDV"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 4] + [1, 2] = [4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sumando con la librería NumPy\n",
        "# con NumPy se manejan ndarray que están preparados para álgebra lineal\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "u = np.array([3, 4])\n",
        "v = np.array([1, 2])\n",
        "\n",
        "w = u + v           # sumando los dos vectores como ndarrays\n",
        "print(f\"{u}  + {v}  = {w}\")"
      ],
      "metadata": {
        "id": "RD4oaT__I3l3",
        "outputId": "45466aa9-9a69-429b-cf50-a3ef15e2375b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4]  + [1 2]  = [4 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resta de Vectores\n",
        "La resta se realiza también componente a componente:\n",
        "\n",
        "$$\\vec{u} - \\vec{v} = (u_1 - v_1, u_2 - v_2, ..., u_n - v_n)$$\n",
        "\n",
        "**Ejemplo:**\n",
        "- $\\vec{u} = (3, 4)$ y $\\vec{v} = (1, 2)$\n",
        "- $\\vec{u} - \\vec{v} = (3-1,\\, 4-2) = (2, 2)$"
      ],
      "metadata": {
        "id": "6TnhrOIrLWl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resta de Vectores con NumPy\n",
        "\n",
        "w = u - v           # restando los dos vectores como ndarrays\n",
        "print(f\"{u}  - {v}  = {w}\")"
      ],
      "metadata": {
        "id": "UqG1azb2JSbE",
        "outputId": "20e17ac7-873b-49d3-8bfd-c5aa47fa885b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4]  - [1 2]  = [2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Método Gráfico\n",
        "\n",
        "#### Suma de Vectores\n",
        "1. Dibujar el primer vector\n",
        "2. Desde el final del primer vector, dibujar el segundo vector\n",
        "3. El vector resultante va desde el origen hasta el final del segundo\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_5.png?raw=1\" alt=\"suma de vectores\" width=\"320\"/>\n",
        "\n",
        "#### Resta de Vectores\n",
        "1. Dibujar el primer vector\n",
        "2. Dibujar el segundo vector en dirección opuesta\n",
        "3. El vector resultante va desde el origen hasta el final del segundo vector\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_6.png?raw=1\" alt=\"resta de vectores\" width=\"380\"/>\n",
        "\n",
        "## Multiplicación por Escalar\n",
        "\n",
        "La multiplicación de un vector por un escalar $k$ multiplica cada componente del vector por ese número:\n",
        "\n",
        "Sea el vector $\\vec{v} = (v_1, v_2, ..., v_n)$ y $k$ el escalar:\n",
        "\n",
        "$$k\\vec{v} = (kv_1, kv_2, ..., kv_n)$$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_7.png?raw=1\" alt=\"producto por escalar\" width=\"280\"/>\n",
        "\n",
        "**Propiedades**\n",
        "1. Distributiva respecto a la suma de vectores: $k(\\vec{u} + \\vec{v}) = k\\vec{u} + k\\vec{v}$\n",
        "2. Distributiva respecto a la suma de escalares:: $(k_1 + k_2)\\vec{v} = k_1\\vec{v} + k_2\\vec{v}$\n",
        "3. Asociativa respecto a la multiplicación de escalares:: $k_1(k_2\\vec{v}) = (k_1k_2)\\vec{v}$\n",
        "4. Identidad: $1\\vec{v} = \\vec{v}$\n",
        "5. La multiplicación por cero da el vector cero: $0\\vec{v} = \\vec{0}$\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "1. **Vector en 2D:**\n",
        "   - **Ejemplo 1:** Multiplicar el vector $\\vec{v} = (3, 4)$ por el escalar $k = 2$.\n",
        "\n",
        "   $$\\vec{w} = 2 \\vec{v} = (2 \\cdot 3,\\, 2 \\cdot 4) = (6, 8)$$\n",
        "\n",
        "   - **Ejemplo 2:** Multiplicar el vector $\\vec{u} = (-2, 1)$ por el escalar $k = -3$.\n",
        "\n",
        "   $$\n",
        "   \\vec{w} = -3 \\vec{u} = (-3 \\cdot -2,\\, -3 \\cdot 1) = (6, -3)\n",
        "   $$\n",
        "\n",
        "2. **Vector en 3D:**\n",
        "   - **Ejemplo:** Multiplicar el vector $\\vec{w} = (-2, 0, 6)$ por el escalar $k = -\\frac{1}{2}$.\n",
        "\n",
        "   $$\\vec{z} =  -\\tfrac{1}{2} \\vec{w} = (( -\\tfrac{1}{2}) \\cdot (-2), \\, ( -\\tfrac{1}{2}) \\cdot 0, \\, ( -\\tfrac{1}{2}) \\cdot 6) = (1, 0, -3)$$"
      ],
      "metadata": {
        "id": "0-Dx_GP8VoWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = np.array([3, 4])\n",
        "w = 2 * v\n",
        "w"
      ],
      "metadata": {
        "id": "4dPhRWVZVsB_",
        "outputId": "1612d3d5-8445-4ff2-ef80-d838cde6a733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Ejemplos en contexto de IA\n",
        "\n",
        "### 1. Actualización de Pesos en Redes Neuronales\n",
        "En el entrenamiento de redes neuronales, los pesos se actualizan mediante:\n",
        "\n",
        "$$\\vec{w}_{\\text{nuevo}} = \\vec{w}_{\\text{actual}} - \\alpha\\nabla\\vec{w}$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{w}$ es el vector de pesos\n",
        "- $\\alpha$ es la tasa de aprendizaje (escalar)\n",
        "- $\\nabla\\vec{w}$ es el gradiente (vector de las derivadas parciales)\n",
        "\n",
        "### 2. Promedio de Vectores de Características\n",
        "Para calcular el centroide de un cluster:\n",
        "\n",
        "$$\\vec{c} = \\frac{1}{n}\\sum_{i=1}^n \\vec{x}_i$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{x}_i$ son los vectores de características\n",
        "- $n$ es el número de vectores\n",
        "- $\\vec{c}$ es el vector\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_8.webp?raw=1\" alt=\"centroides de clusters\" width=\"420\"/>\n",
        "\n",
        "### 3. Combinación de Embeddings\n",
        "En procesamiento de lenguaje natural:\n",
        "\n",
        "$$\\vec{v}_{\\text{frase}} = \\frac{\\vec{v}_{\\text{palabra1}} + \\vec{v}_{\\text{palabra2}} + ... + \\vec{v}_{\\text{palabraN}}}{N}$$\n",
        "\n",
        "### 4. Normalización de Datos\n",
        "Escalado de vectores de características:\n",
        "\n",
        "$$\\vec{x}_{\\text{normalizado}} = \\frac{\\vec{x} - \\mu}{\\sigma}$$\n",
        "\n",
        "Donde:\n",
        "- $\\mu$ es la media\n",
        "- $\\sigma$ es la desviación estándar\n",
        "\n",
        "Estas operaciones básicas son fundamentales en muchos algoritmos de IA, desde el preprocesamiento de datos hasta el entrenamiento de modelos complejos."
      ],
      "metadata": {
        "id": "jJE7MuRaXdDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Propiedades Fundamentales\n",
        "\n",
        "## Norma\n",
        "\n",
        "La norma o magnitud de un vector $\\vec{v}$ es una medida de su longitud. En el contexto de machine learning, representa la \"fuerza\" o \"intensidad\" de un vector de características.\n",
        "\n",
        "### Definición Formal\n",
        "Para un vector $\\vec{v} = (v_1, v_2, ..., v_n)$, la norma euclidiana (L2) se define como:\n",
        "\n",
        "$$\\|\\vec{v}\\| = \\sqrt{\\sum_{i=1}^n v_i^2}$$\n",
        "\n",
        "**Ejemplos comunes:**\n",
        "- En 2D: $\\|\\vec{v}\\| = \\sqrt{v_1^2 + v_2^2}$\n",
        "    - Si $\\vec{v} = (3, -4)$ entonces su norma es $\\|\\vec{v}\\| = \\sqrt{3^2 + (-4)^2}= \\sqrt{9 + 16} = \\sqrt{25} = 5$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_9.png?raw=1\" alt=\"norma de un vector 2D\" width=\"280\"/>\n",
        "\n",
        "- En 3D: $\\|\\vec{v}\\| = \\sqrt{v_1^2 + v_2^2 + v_3^2}$\n",
        "\n",
        "### Otras Normas Comunes en ML\n",
        "1. **Norma L1 (Manhattan):**\n",
        "   $$\\|\\vec{v}\\|_1 = \\sum_{i=1}^n |v_i|$$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_10.png?raw=1\" alt=\"norma Manhattan\" width=\"320\"/>\n",
        "\n",
        "2. **Norma L∞ (Máximo):**\n",
        "   $$\\|\\vec{v}\\|_∞ = \\max_{i} |v_i|$$\n",
        "\n",
        "## Distancia entre Puntos\n",
        "\n",
        "La distancia entre dos puntos es crucial en muchos algoritmos de ML, especialmente en clustering y clasificación.\n",
        "\n",
        "### Distancia Euclidiana\n",
        "Para dos vectores $\\vec{u}$ y $\\vec{v}$:\n",
        "\n",
        "$$d(\\vec{u}, \\vec{v}) = \\|\\vec{u} - \\vec{v}\\| = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}$$\n",
        "\n",
        "### Otras Métricas de Distancia en ML\n",
        "1. **Distancia Manhattan:**\n",
        "   $$d_1(\\vec{u}, \\vec{v}) = \\sum_{i=1}^n |u_i - v_i|$$\n",
        "\n",
        "2. **Distancia de Coseno:**\n",
        "   $$d_{\\cos}(\\vec{u}, \\vec{v}) = 1 - \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "## Normalización y Vectores Unitarios\n",
        "\n",
        "### Vector Unitario\n",
        "Un vector unitario tiene norma igual a 1. Se obtiene dividiendo un vector por su norma:\n",
        "\n",
        "$$\\hat{v} = \\frac{\\vec{v}}{\\|\\vec{v}\\|}$$\n",
        "\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "1. **Vector en 2D:**\n",
        "   - **Ejemplo 1:** Normalizar el vector $\\vec{v} = (3, 4)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{v}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{v}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{v}}{\\|\\vec{v}\\|} = \\frac{(3, 4)}{5} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)\n",
        "   $$\n",
        "\n",
        "   - **Ejemplo 2:** Normalizar el vector $\\vec{u} = (-2, 1)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{u}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{u}\\| = \\sqrt{(-2)^2 + 1^2} = \\sqrt{4 + 1} = \\sqrt{5}\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{u}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{u}}{\\|\\vec{u}\\|} = \\frac{(-2, 1)}{\\sqrt{5}} = \\left(\\frac{-2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}}\\right)\n",
        "   $$\n",
        "\n",
        "2. **Vector en 3D:**\n",
        "   - **Ejemplo 1:** Normalizar el vector $\\vec{w} = (1, 2, 3)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{w}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{w}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{w}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{w}}{\\|\\vec{w}\\|} = \\frac{(1, 2, 3)}{\\sqrt{14}} = \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}}, \\frac{3}{\\sqrt{14}}\\right)\n",
        "   $$\n",
        "\n",
        "   - **Ejemplo 2:** Normalizar el vector $\\vec{z} = (0, -1, 2)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{z}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{z}\\| = \\sqrt{0^2 + (-1)^2 + 2^2} = \\sqrt{0 + 1 + 4} = \\sqrt{5}\n",
        "   $$\n",
        "\n",
        "### Proceso de Normalización\n",
        "1. **Min-Max Scaling:**\n",
        "   $$x_{\\text{norm}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
        "\n",
        "2. **Z-Score Normalization:**\n",
        "   $$x_{\\text{norm}} = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "## Aplicaciones en Machine Learning\n",
        "\n",
        "### 1. Preprocesamiento de Datos\n",
        "- **Normalización de características:** Asegura que todas las características contribuyan equitativamente al modelo\n",
        "```python\n",
        "X_normalized = (X - X.mean()) / X.std()\n",
        "```\n",
        "\n",
        "### 2. Medición de Similitud\n",
        "- **Similitud coseno:** Usada en sistemas de recomendación y procesamiento de texto\n",
        "```python\n",
        "similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "```\n",
        "\n",
        "### 3. Algoritmos de Clustering\n",
        "- **K-means:** Usa distancia euclidiana para agrupar puntos\n",
        "```python\n",
        "distance = np.linalg.norm(point - centroid)\n",
        "```\n",
        "\n",
        "### 4. Regularización\n",
        "- **Norma L1 y L2:** Usadas para penalizar pesos grandes en modelos\n",
        "```python\n",
        "L1_penalty = np.sum(np.abs(weights))\n",
        "L2_penalty = np.sqrt(np.sum(weights**2))\n",
        "```\n",
        "\n",
        "### 5. Redes Neuronales\n",
        "- **Normalización de capas:** Mejora la estabilidad del entrenamiento\n",
        "```python\n",
        "layer_norm = (layer - layer.mean()) / layer.std()\n",
        "```\n",
        "\n",
        "### 6. Embeddings\n",
        "- **Normalización de word embeddings:** Facilita la comparación de similitud entre palabras\n",
        "```python\n",
        "normalized_embedding = embedding / np.linalg.norm(embedding)\n",
        "```\n",
        "\n",
        "Las propiedades fundamentales de los vectores son esenciales en machine learning porque:\n",
        "- Permiten cuantificar similitudes y diferencias entre datos\n",
        "- Facilitan la optimización de modelos\n",
        "- Mejoran la estabilidad numérica\n",
        "- Ayudan en la interpretación de resultados\n",
        "\n",
        "El entendimiento de estas propiedades es crucial para:\n",
        "- Diseñar y ajustar modelos efectivamente\n",
        "- Elegir las métricas apropiadas\n",
        "- Interpretar resultados correctamente\n",
        "- Optimizar el rendimiento del modelo"
      ],
      "metadata": {
        "id": "i-7rcop1XdDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Productos entre Vectores\n",
        "\n",
        "## Producto Escalar (Dot Product)\n",
        "\n",
        "El producto escalar es una operación fundamental en deep learning que produce un escalar a partir de dos vectores.\n",
        "\n",
        "### Definición\n",
        "Para dos vectores $\\vec{u} = (u_1, u_2, ..., u_n)$ y $\\vec{v} = (v_1, v_2, ..., v_n)$:\n",
        "\n",
        "$$\\vec{u} \\cdot \\vec{v} = \\sum_{i=1}^n u_i v_i = u_1v_1 + u_2v_2 + ... + u_nv_n$$\n",
        "\n",
        "**Ejemplo**\n",
        "\n",
        "Calcular el producto escalar de $\\vec{u} = (1, 2, 3)$ y $\\vec{v} = (-4, 5, 6)$.\n",
        "\n",
        "   $$\n",
        "   \\vec{u} \\cdot \\vec{v} = 1 \\cdot (-4) + 2 \\cdot 5 + 3 \\cdot 6 = -4 + 10 + 18 = 24\n",
        "   $$"
      ],
      "metadata": {
        "id": "yqosGlIwXdDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "v1 = np.array([1,2,3])\n",
        "v2 = np.array([-4,5,6])     # v1 y v2 deben ser de la misma longitud\n",
        "print(f\"Producto punto: 1*(-4) + 2*5 + 3*6 = {1*(-4) + 2*5 + 3*6}\")\n",
        "print(v1.dot(v2))   # forma clásica de hacer el producto punto de dos vectores\n",
        "v1 @ v2             # la @ funciona desde la versión 3.5 de Python"
      ],
      "metadata": {
        "outputId": "07ff4e2c-36b3-4a43-81a8-bdd7975c156b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1V1oXp6XdDW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producto punto: 1*(-4) + 2*5 + 3*6 = 24\n",
            "24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades\n",
        "1. **Conmutativa:** $\\vec{u} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{u}$\n",
        "2. **Distributiva:** $\\vec{u} \\cdot (\\vec{v} + \\vec{w}) = \\vec{u} \\cdot \\vec{v} + \\vec{u} \\cdot \\vec{w}$\n",
        "3. **Asociativa respecto a la multiplicación por escalar:** $(k\\vec{u}) \\cdot \\vec{v} = k(\\vec{u} \\cdot \\vec{v})$\n",
        "4. **Producto escalar de un vector consigo mismo:** $\\vec{v} \\cdot \\vec{v} = \\|\\vec{v}\\|^2$\n",
        "\n",
        "### Ángulo entre Vectores\n",
        "El producto escalar está relacionado con el ángulo $\\theta$ entre dos vectores:\n",
        "\n",
        "$$\\cos \\theta = \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "## Producto Vectorial (Cross Product)\n",
        "\n",
        "El producto vectorial es menos común en deep learning, pero es importante en ciertas aplicaciones de visión por computador y gráficos.\n",
        "\n",
        "### Definición\n",
        "Para vectores en 3D, $\\vec{u} = (u_1, u_2, u_3)$ y $\\vec{v} = (v_1, v_2, v_3)$:\n",
        "\n",
        "$$\\vec{u} \\times \\vec{v} = (u_2v_3 - u_3v_2, u_3v_1 - u_1v_3, u_1v_2 - u_2v_1)$$\n",
        "\n",
        "### Propiedades\n",
        "1. **Anticonmutativa:** $\\vec{u} \\times \\vec{v} = -(\\vec{v} \\times \\vec{u})$\n",
        "2. **Distributiva:** $\\vec{u} \\times (\\vec{v} + \\vec{w}) = \\vec{u} \\times \\vec{v} + \\vec{u} \\times \\vec{w}$\n",
        "3. **Escalar:** $(k\\vec{u}) \\times \\vec{v} = k(\\vec{u} \\times \\vec{v})$\n",
        "\n",
        "## Aplicaciones en Deep Learning\n",
        "\n",
        "### 1. Capas Densas (Fully Connected)\n",
        "El producto escalar es la operación básica en las capas densas:\n",
        "\n",
        "$$y = \\vec{w} \\cdot \\vec{x} + b$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{w}$ es el vector de pesos\n",
        "- $\\vec{x}$ es el vector de entrada\n",
        "- $b$ es el bias\n",
        "- $y$ es la salida\n",
        "\n",
        "### 2. Atención y Self-Attention\n",
        "```python\n",
        "# Cálculo de scores de atención\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "```\n",
        "\n",
        "### 3. Similitud Coseno\n",
        "Usada en recuperación de información y sistemas de recomendación:\n",
        "\n",
        "$$\\text{similitud} = \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "### 4. Convoluciones\n",
        "Las operaciones de convolución utilizan productos escalares:\n",
        "```python\n",
        "# Convolución 2D simplificada\n",
        "output = torch.sum(input * kernel)\n",
        "```\n",
        "\n",
        "### 5. Métricas de Rendimiento\n",
        "El producto escalar se usa en métricas como:\n",
        "- Precisión coseno\n",
        "- Distancia euclidiana al cuadrado\n",
        "```python\n",
        "distance = torch.sum((x - y)**2)\n",
        "```\n",
        "\n",
        "### 6. Backpropagation\n",
        "Los gradientes se calculan usando productos escalares:\n",
        "```python\n",
        "# Gradiente simplificado\n",
        "gradient = torch.matmul(error, weights.T)\n",
        "```\n",
        "\n",
        "El producto escalar es especialmente importante en deep learning porque:\n",
        "- Es computacionalmente eficiente\n",
        "- Se paraleliza bien en GPUs\n",
        "- Es la base de muchas operaciones de red neuronal\n",
        "- Permite medir similitudes y diferencias\n",
        "\n",
        "El producto vectorial es menos común pero útil en:\n",
        "- Procesamiento de datos 3D\n",
        "- Visión por computador\n",
        "- Gráficos por computador\n",
        "- Robótica y control\n",
        "\n",
        "La comprensión de estas operaciones es fundamental para:\n",
        "- Implementar redes neuronales\n",
        "- Optimizar el rendimiento\n",
        "- Diseñar nuevas arquitecturas\n",
        "- Depurar problemas de entrenamiento"
      ],
      "metadata": {
        "id": "qE6alk8LXdDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Operaciones Básicas\n",
        "\n",
        "## Suma y Resta de Vectores\n",
        "\n",
        "Las operaciones de suma y resta de vectores son fundamentales en IA, especialmente en el procesamiento de datos y en el entrenamiento de modelos.\n",
        "\n",
        "### Método Algebraico\n",
        "\n",
        "#### Suma de Vectores\n",
        "\n",
        "La suma de dos vectores se realiza componente a componente:\n",
        "\n",
        "Si $\\vec{u} = (u_1, u_2, ..., u_n)$ y $\\vec{v} = (v_1, v_2, ..., v_n)$, entonces:\n",
        "\n",
        "$$\\vec{u} + \\vec{v} = (u_1 + v_1, u_2 + v_2, ..., u_n + v_n)$$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_5.png?raw=1\" alt=\"suma de vectores\" width=\"320\"/>\n",
        "\n",
        "**Ejemplo:**\n",
        "- $\\vec{u} = (3, 4)$ y $\\vec{v} = (1, 2)$\n",
        "- $\\vec{u} + \\vec{v} = (3+1, 4+2) = (4, 6)$"
      ],
      "metadata": {
        "id": "gqDWZpZVWEiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u = np.array([3, 4])\n",
        "v = np.array([1, 2])\n",
        "print(np.sum([u, v]))           # sumando los dos vectores como ndarrays\n",
        "\n",
        "print(np.sum([[3, 4], [1, 2]])) # sumando los elementos de una lista de listas\n",
        "\n",
        "print(np.sum([u, v], axis=0))   # [4,6] sumando los elementos de cada columna\n",
        "print(np.sum(np.sum([u, v], axis=0)))   # sumando lo anterior con otro np.sum\n",
        "\n",
        "print(np.sum([u, v], axis=1))   # [7,3] sumando los elementos de cada fila\n",
        "print(np.sum(np.sum([u, v], axis=1)))   # sumando lo anterior con otro np.sum"
      ],
      "metadata": {
        "id": "EXUdjS2aWJOs",
        "outputId": "a319a1bb-0ec9-41ba-fb7e-d844b8574265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n",
            "[4 6]\n",
            "10\n",
            "[7 3]\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Resta de Vectores\n",
        "La resta se realiza también componente a componente:\n",
        "\n",
        "$$\\vec{u} - \\vec{v} = (u_1 - v_1, u_2 - v_2, ..., u_n - v_n)$$\n",
        "\n",
        "**Ejemplo:**\n",
        "- $\\vec{u} = (3, 4)$ y $\\vec{v} = (1, 2)$\n",
        "- $\\vec{u} - \\vec{v} = (3-1, 4-2) = (2, 2)$\n",
        "\n",
        "### Método Gráfico\n",
        "\n",
        "#### Suma de Vectores\n",
        "1. Dibujar el primer vector\n",
        "2. Desde el final del primer vector, dibujar el segundo vector\n",
        "3. El vector resultante va desde el origen hasta el final del segundo vector\n",
        "\n",
        "#### Resta de Vectores\n",
        "1. Dibujar el primer vector\n",
        "2. Dibujar el segundo vector en dirección opuesta\n",
        "3. El vector resultante va desde el origen hasta el final del segundo vector\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_6.png?raw=1\" alt=\"resta de vectores\" width=\"320\"/>\n",
        "\n",
        "## Multiplicación por Escalar\n",
        "\n",
        "La multiplicación de un vector por un escalar $k$ multiplica cada componente del vector por ese número:\n",
        "\n",
        "Sea el vector $\\vec{v} = (v_1, v_2, ..., v_n)$ y $k$ el escalar:\n",
        "\n",
        "$$k\\vec{v} = (kv_1, kv_2, ..., kv_n)$$\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_7.png?raw=1\" alt=\"producto por escalar\" width=\"280\"/>\n",
        "\n",
        "**Propiedades**\n",
        "1. Distributiva respecto a la suma de vectores: $k(\\vec{u} + \\vec{v}) = k\\vec{u} + k\\vec{v}$\n",
        "2. Distributiva respecto a la suma de escalares:: $(k_1 + k_2)\\vec{v} = k_1\\vec{v} + k_2\\vec{v}$\n",
        "3. Asociativa respecto a la multiplicación de escalares:: $k_1(k_2\\vec{v}) = (k_1k_2)\\vec{v}$\n",
        "4. Identidad: $1\\vec{v} = \\vec{v}$\n",
        "5. La multiplicación por cero da el vector cero: $0\\vec{v} = \\vec{0}$\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "1. **Vector en 2D:**\n",
        "   - **Ejemplo 1:** Multiplicar el vector $\\vec{v} = (3, 4)$ por el escalar $k = 2$.\n",
        "\n",
        "   $$\n",
        "   \\vec{w} = 2 \\vec{v} = (2 \\cdot 3, 2 \\cdot 4) = (6, 8)\n",
        "   $$\n",
        "\n",
        "   - **Ejemplo 2:** Multiplicar el vector $\\vec{u} = (-2, 1)$ por el escalar $k = -3$.\n",
        "\n",
        "   $$\n",
        "   \\vec{w} = -3 \\vec{u} = (-3 \\cdot -2, -3 \\cdot 1) = (6, -3)\n",
        "   $$\n",
        "\n",
        "2. **Vector en 3D:**\n",
        "   - **Ejemplo:** Multiplicar el vector $\\vec{w} = (-2, 0, 6)$ por el escalar $k = -\\frac{1}{2}$.\n",
        "\n",
        "   $$\\vec{z} =  -\\tfrac{1}{2} \\vec{w} = (( -\\tfrac{1}{2}) \\cdot (-2), \\, ( -\\tfrac{1}{2}) \\cdot 0, \\, ( -\\tfrac{1}{2}) \\cdot 6) = (1, 0, -3)$$\n",
        "\n",
        "## Ejemplos en contexto de IA\n",
        "\n",
        "### 1. Actualización de Pesos en Redes Neuronales\n",
        "En el entrenamiento de redes neuronales, los pesos se actualizan mediante:\n",
        "\n",
        "$$\\vec{w}_{\\text{nuevo}} = \\vec{w}_{\\text{actual}} - \\alpha\\nabla\\vec{w}$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{w}$ es el vector de pesos\n",
        "- $\\alpha$ es la tasa de aprendizaje (escalar)\n",
        "- $\\nabla\\vec{w}$ es el gradiente\n",
        "\n",
        "### 2. Promedio de Vectores de Características\n",
        "Para calcular el centroide de un cluster:\n",
        "\n",
        "$$\\vec{c} = \\frac{1}{n}\\sum_{i=1}^n \\vec{x}_i$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{x}_i$ son los vectores de características\n",
        "- $n$ es el número de vectores\n",
        "- $\\vec{c}$ es el vector\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_8.webp?raw=1\" alt=\"centroides de clusters\" width=\"420\"/>\n",
        "\n",
        "### 3. Combinación de Embeddings\n",
        "En procesamiento de lenguaje natural:\n",
        "\n",
        "$$\\vec{v}_{\\text{frase}} = \\frac{\\vec{v}_{\\text{palabra1}} + \\vec{v}_{\\text{palabra2}} + ... + \\vec{v}_{\\text{palabraN}}}{N}$$\n",
        "\n",
        "### 4. Normalización de Datos\n",
        "Escalado de vectores de características:\n",
        "\n",
        "$$\\vec{x}_{\\text{normalizado}} = \\frac{\\vec{x} - \\mu}{\\sigma}$$\n",
        "\n",
        "Donde:\n",
        "- $\\mu$ es la media\n",
        "- $\\sigma$ es la desviación estándar\n",
        "\n",
        "Estas operaciones básicas son fundamentales en muchos algoritmos de IA, desde el preprocesamiento de datos hasta el entrenamiento de modelos complejos."
      ],
      "metadata": {
        "id": "8s0dyRnVDDTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Propiedades Fundamentales\n",
        "\n",
        "## Norma\n",
        "\n",
        "La norma o magnitud de un vector $\\vec{v}$ es una medida de su longitud. En el contexto de machine learning, representa la \"fuerza\" o \"intensidad\" de un vector de características.\n",
        "\n",
        "### Definición Formal\n",
        "Para un vector $\\vec{v} = (v_1, v_2, ..., v_n)$, la norma euclidiana (L2) se define como:\n",
        "\n",
        "$$\\|\\vec{v}\\| = \\sqrt{\\sum_{i=1}^n v_i^2}$$\n",
        "\n",
        "**Ejemplos comunes:**\n",
        "- En 2D: $\\|\\vec{v}\\| = \\sqrt{v_1^2 + v_2^2}$\n",
        "    - Si $\\vec{v} = (3, -4)$ entonces su norma es $\\|\\vec{v}\\| = \\sqrt{3^2 + (-4)^2}= \\sqrt{9 + 16} = \\sqrt{25} = 5$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_9.png?raw=1\" alt=\"norma de un vector 2D\" width=\"280\"/>\n",
        "\n",
        "- En 3D: $\\|\\vec{v}\\| = \\sqrt{v_1^2 + v_2^2 + v_3^2}$\n",
        "\n",
        "### Otras Normas Comunes en ML\n",
        "1. **Norma L1 (Manhattan):**\n",
        "   $$\\|\\vec{v}\\|_1 = \\sum_{i=1}^n |v_i|$$\n",
        "\n",
        "<img src=\"https://github.com/financieras/math_for_ai/blob/main/img/vector_10.png?raw=1\" alt=\"norma Manhattan\" width=\"320\"/>\n",
        "\n",
        "2. **Norma L∞ (Máximo):**\n",
        "   $$\\|\\vec{v}\\|_∞ = \\max_{i} |v_i|$$\n",
        "\n",
        "## Distancia entre Puntos\n",
        "\n",
        "La distancia entre dos puntos es crucial en muchos algoritmos de ML, especialmente en clustering y clasificación.\n",
        "\n",
        "### Distancia Euclidiana\n",
        "Para dos vectores $\\vec{u}$ y $\\vec{v}$:\n",
        "\n",
        "$$d(\\vec{u}, \\vec{v}) = \\|\\vec{u} - \\vec{v}\\| = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}$$\n",
        "\n",
        "### Otras Métricas de Distancia en ML\n",
        "1. **Distancia Manhattan:**\n",
        "   $$d_1(\\vec{u}, \\vec{v}) = \\sum_{i=1}^n |u_i - v_i|$$\n",
        "\n",
        "2. **Distancia de Coseno:**\n",
        "   $$d_{\\cos}(\\vec{u}, \\vec{v}) = 1 - \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "## Normalización y Vectores Unitarios\n",
        "\n",
        "### Vector Unitario\n",
        "Un vector unitario tiene norma igual a 1. Se obtiene dividiendo un vector por su norma:\n",
        "\n",
        "$$\\hat{v} = \\frac{\\vec{v}}{\\|\\vec{v}\\|}$$\n",
        "\n",
        "\n",
        "**Ejemplos**\n",
        "\n",
        "1. **Vector en 2D:**\n",
        "   - **Ejemplo 1:** Normalizar el vector $\\vec{v} = (3, 4)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{v}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{v}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{v}}{\\|\\vec{v}\\|} = \\frac{(3, 4)}{5} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)\n",
        "   $$\n",
        "\n",
        "   - **Ejemplo 2:** Normalizar el vector $\\vec{u} = (-2, 1)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{u}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{u}\\| = \\sqrt{(-2)^2 + 1^2} = \\sqrt{4 + 1} = \\sqrt{5}\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{u}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{u}}{\\|\\vec{u}\\|} = \\frac{(-2, 1)}{\\sqrt{5}} = \\left(\\frac{-2}{\\sqrt{5}}, \\frac{1}{\\sqrt{5}}\\right)\n",
        "   $$\n",
        "\n",
        "2. **Vector en 3D:**\n",
        "   - **Ejemplo 1:** Normalizar el vector $\\vec{w} = (1, 2, 3)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{w}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{w}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n",
        "   $$\n",
        "\n",
        "   Luego, dividimos $\\vec{w}$ por su norma:\n",
        "\n",
        "   $$\n",
        "   \\hat{u} = \\frac{\\vec{w}}{\\|\\vec{w}\\|} = \\frac{(1, 2, 3)}{\\sqrt{14}} = \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}}, \\frac{3}{\\sqrt{14}}\\right)\n",
        "   $$\n",
        "\n",
        "   - **Ejemplo 2:** Normalizar el vector $\\vec{z} = (0, -1, 2)$.\n",
        "\n",
        "   Primero, calculamos la norma de $\\vec{z}$:\n",
        "\n",
        "   $$\n",
        "   \\|\\vec{z}\\| = \\sqrt{0^2 + (-1)^2 + 2^2} = \\sqrt{0 + 1 + 4} = \\sqrt{5}\n",
        "   $$\n",
        "\n",
        "### Proceso de Normalización\n",
        "1. **Min-Max Scaling:**\n",
        "   $$x_{\\text{norm}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
        "\n",
        "2. **Z-Score Normalization:**\n",
        "   $$x_{\\text{norm}} = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "## Aplicaciones en Machine Learning\n",
        "\n",
        "### 1. Preprocesamiento de Datos\n",
        "- **Normalización de características:** Asegura que todas las características contribuyan equitativamente al modelo\n",
        "```python\n",
        "X_normalized = (X - X.mean()) / X.std()\n",
        "```\n",
        "\n",
        "### 2. Medición de Similitud\n",
        "- **Similitud coseno:** Usada en sistemas de recomendación y procesamiento de texto\n",
        "```python\n",
        "similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "```\n",
        "\n",
        "### 3. Algoritmos de Clustering\n",
        "- **K-means:** Usa distancia euclidiana para agrupar puntos\n",
        "```python\n",
        "distance = np.linalg.norm(point - centroid)\n",
        "```\n",
        "\n",
        "### 4. Regularización\n",
        "- **Norma L1 y L2:** Usadas para penalizar pesos grandes en modelos\n",
        "```python\n",
        "L1_penalty = np.sum(np.abs(weights))\n",
        "L2_penalty = np.sqrt(np.sum(weights**2))\n",
        "```\n",
        "\n",
        "### 5. Redes Neuronales\n",
        "- **Normalización de capas:** Mejora la estabilidad del entrenamiento\n",
        "```python\n",
        "layer_norm = (layer - layer.mean()) / layer.std()\n",
        "```\n",
        "\n",
        "### 6. Embeddings\n",
        "- **Normalización de word embeddings:** Facilita la comparación de similitud entre palabras\n",
        "```python\n",
        "normalized_embedding = embedding / np.linalg.norm(embedding)\n",
        "```\n",
        "\n",
        "Las propiedades fundamentales de los vectores son esenciales en machine learning porque:\n",
        "- Permiten cuantificar similitudes y diferencias entre datos\n",
        "- Facilitan la optimización de modelos\n",
        "- Mejoran la estabilidad numérica\n",
        "- Ayudan en la interpretación de resultados\n",
        "\n",
        "El entendimiento de estas propiedades es crucial para:\n",
        "- Diseñar y ajustar modelos efectivamente\n",
        "- Elegir las métricas apropiadas\n",
        "- Interpretar resultados correctamente\n",
        "- Optimizar el rendimiento del modelo"
      ],
      "metadata": {
        "id": "Vr2S1kEMGncS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Productos entre Vectores\n",
        "\n",
        "## Producto Escalar (Dot Product)\n",
        "\n",
        "El producto escalar es una operación fundamental en deep learning que produce un escalar a partir de dos vectores.\n",
        "\n",
        "### Definición\n",
        "Para dos vectores $\\vec{u} = (u_1, u_2, ..., u_n)$ y $\\vec{v} = (v_1, v_2, ..., v_n)$:\n",
        "\n",
        "$$\\vec{u} \\cdot \\vec{v} = \\sum_{i=1}^n u_i v_i = u_1v_1 + u_2v_2 + ... + u_nv_n$$\n",
        "\n",
        "**Ejemplo**\n",
        "\n",
        "Calcular el producto escalar de $\\vec{u} = (1, 2, 3)$ y $\\vec{v} = (-4, 5, 6)$.\n",
        "\n",
        "   $$\n",
        "   \\vec{u} \\cdot \\vec{v} = 1 \\cdot (-4) + 2 \\cdot 5 + 3 \\cdot 6 = -4 + 10 + 18 = 24\n",
        "   $$"
      ],
      "metadata": {
        "id": "BAWoUiSu_YB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "v1 = np.array([1,2,3])\n",
        "v2 = np.array([-4,5,6])     # v1 y v2 deben ser de la misma longitud\n",
        "print(f\"Producto punto: 1*(-4) + 2*5 + 3*6 = {1*(-4) + 2*5 + 3*6}\")\n",
        "print(v1.dot(v2))   # forma clásica de hacer el producto punto de dos vectores\n",
        "v1 @ v2             # la @ funciona desde la versión 3.5 de Python"
      ],
      "metadata": {
        "id": "2neYbIJ9_lGP",
        "outputId": "07ff4e2c-36b3-4a43-81a8-bdd7975c156b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Producto punto: 1*(-4) + 2*5 + 3*6 = 24\n",
            "24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades\n",
        "1. **Conmutativa:** $\\vec{u} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{u}$\n",
        "2. **Distributiva:** $\\vec{u} \\cdot (\\vec{v} + \\vec{w}) = \\vec{u} \\cdot \\vec{v} + \\vec{u} \\cdot \\vec{w}$\n",
        "3. **Asociativa respecto a la multiplicación por escalar:** $(k\\vec{u}) \\cdot \\vec{v} = k(\\vec{u} \\cdot \\vec{v})$\n",
        "4. **Producto escalar de un vector consigo mismo:** $\\vec{v} \\cdot \\vec{v} = \\|\\vec{v}\\|^2$\n",
        "\n",
        "### Ángulo entre Vectores\n",
        "El producto escalar está relacionado con el ángulo $\\theta$ entre dos vectores:\n",
        "\n",
        "$$\\cos \\theta = \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "## Producto Vectorial (Cross Product)\n",
        "\n",
        "El producto vectorial es menos común en deep learning, pero es importante en ciertas aplicaciones de visión por computador y gráficos.\n",
        "\n",
        "### Definición\n",
        "Para vectores en 3D, $\\vec{u} = (u_1, u_2, u_3)$ y $\\vec{v} = (v_1, v_2, v_3)$:\n",
        "\n",
        "$$\\vec{u} \\times \\vec{v} = (u_2v_3 - u_3v_2, u_3v_1 - u_1v_3, u_1v_2 - u_2v_1)$$\n",
        "\n",
        "### Propiedades\n",
        "1. **Anticonmutativa:** $\\vec{u} \\times \\vec{v} = -(\\vec{v} \\times \\vec{u})$\n",
        "2. **Distributiva:** $\\vec{u} \\times (\\vec{v} + \\vec{w}) = \\vec{u} \\times \\vec{v} + \\vec{u} \\times \\vec{w}$\n",
        "3. **Escalar:** $(k\\vec{u}) \\times \\vec{v} = k(\\vec{u} \\times \\vec{v})$\n",
        "\n",
        "## Aplicaciones en Deep Learning\n",
        "\n",
        "### 1. Capas Densas (Fully Connected)\n",
        "El producto escalar es la operación básica en las capas densas:\n",
        "\n",
        "$$y = \\vec{w} \\cdot \\vec{x} + b$$\n",
        "\n",
        "Donde:\n",
        "- $\\vec{w}$ es el vector de pesos\n",
        "- $\\vec{x}$ es el vector de entrada\n",
        "- $b$ es el bias\n",
        "- $y$ es la salida\n",
        "\n",
        "### 2. Atención y Self-Attention\n",
        "```python\n",
        "# Cálculo de scores de atención\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "```\n",
        "\n",
        "### 3. Similitud Coseno\n",
        "Usada en recuperación de información y sistemas de recomendación:\n",
        "\n",
        "$$\\text{similitud} = \\frac{\\vec{u} \\cdot \\vec{v}}{\\|\\vec{u}\\| \\|\\vec{v}\\|}$$\n",
        "\n",
        "### 4. Convoluciones\n",
        "Las operaciones de convolución utilizan productos escalares:\n",
        "```python\n",
        "# Convolución 2D simplificada\n",
        "output = torch.sum(input * kernel)\n",
        "```\n",
        "\n",
        "### 5. Métricas de Rendimiento\n",
        "El producto escalar se usa en métricas como:\n",
        "- Precisión coseno\n",
        "- Distancia euclidiana al cuadrado\n",
        "```python\n",
        "distance = torch.sum((x - y)**2)\n",
        "```\n",
        "\n",
        "### 6. Backpropagation\n",
        "Los gradientes se calculan usando productos escalares:\n",
        "```python\n",
        "# Gradiente simplificado\n",
        "gradient = torch.matmul(error, weights.T)\n",
        "```\n",
        "\n",
        "El producto escalar es especialmente importante en deep learning porque:\n",
        "- Es computacionalmente eficiente\n",
        "- Se paraleliza bien en GPUs\n",
        "- Es la base de muchas operaciones de red neuronal\n",
        "- Permite medir similitudes y diferencias\n",
        "\n",
        "El producto vectorial es menos común pero útil en:\n",
        "- Procesamiento de datos 3D\n",
        "- Visión por computador\n",
        "- Gráficos por computador\n",
        "- Robótica y control\n",
        "\n",
        "La comprensión de estas operaciones es fundamental para:\n",
        "- Implementar redes neuronales\n",
        "- Optimizar el rendimiento\n",
        "- Diseñar nuevas arquitecturas\n",
        "- Depurar problemas de entrenamiento"
      ],
      "metadata": {
        "id": "yqZ-8hysQO7s"
      }
    }
  ]
}