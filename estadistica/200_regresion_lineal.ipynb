{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrJZ7lZTh/F4RlyGIdPQhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/estadistica/200_regresion_lineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBmCJS5SWuTy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión Lineal para Data Science con Python\n",
        "\n",
        "## Contenido\n",
        "\n",
        "**1. Introducción**\n",
        "   - Importancia de la regresión lineal en Data Science\n",
        "   - ¿Qué problema resuelve el método de mínimos cuadrados?\n",
        "\n",
        "**2. Fundamento Matemático**\n",
        "   - Formulación del problema: minimizar el error cuadrático\n",
        "   - Derivación de la solución analítica (ecuaciones normales)\n",
        "   - Notación matricial\n",
        "\n",
        "**3. Implementación desde Cero en Python**\n",
        "   - Dataset sintético de ejemplo\n",
        "   - Cálculo manual con NumPy\n",
        "   - Visualización con Matplotlib\n",
        "\n",
        "**4. Comparación con Scikit-learn**\n",
        "   - Uso de `LinearRegression()`\n",
        "   - Verificación de resultados\n",
        "\n",
        "**5. Métricas de Evaluación**\n",
        "   - MSE, RMSE, R²\n",
        "   - Interpretación práctica\n",
        "\n",
        "**6. Mínimos Cuadrados vs. Descenso del Gradiente**\n",
        "   - Convexidad de la función de coste en regresión lineal\n",
        "   - Solución analítica vs. solución iterativa\n",
        "   - Cuándo usar Descenso del Gradiente: funciones no convexas y múltiples mínimos locales\n",
        "   - Relevancia en Deep Learning y modelos complejos\n",
        "\n",
        "**7. Limitaciones y Consideraciones**\n",
        "   - Supuestos del modelo\n",
        "   - Cuándo usar (y no usar) regresión lineal\n",
        "\n",
        "**8. Conclusiones**\n",
        "   - Aplicaciones en proyectos reales de Data Science"
      ],
      "metadata": {
        "id": "E84_4hGqWu0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introducción\n",
        "\n",
        "Si estás empezando en Data Science, probablemente hayas escuchado que la regresión lineal es uno de los algoritmos más fundamentales. Pero ¿por qué sigue siendo tan relevante en la era del machine learning avanzado y las redes neuronales?\n",
        "\n",
        "La respuesta es simple: **efectividad y interpretabilidad**. Mientras que modelos más complejos pueden actuar como \"cajas negras\", la regresión lineal nos permite entender exactamente cómo cada variable afecta a nuestro resultado. Es nuestro punto de partida natural para cualquier problema de predicción numérica.\n",
        "\n",
        "## ¿Qué problema resuelve realmente?\n",
        "\n",
        "Imagina que tienes datos históricos de precios de viviendas y quieres predecir cuánto costará una nueva casa. Tienes variables como metros cuadrados, número de habitaciones, ubicación... La regresión lineal te permite encontrar una relación matemática entre estas características y el precio.\n",
        "\n",
        "**El método de mínimos cuadrados** es la magia detrás de esto: encuentra la línea (o hiperplano) que mejor se ajusta a nuestros datos, minimizando la distancia total entre los puntos reales y nuestras predicciones.\n",
        "\n",
        "```python\n",
        "# Un vistazo rápido de lo que haremos\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Datos de ejemplo: tamaño de casa vs precio\n",
        "tamaño = np.array([50, 60, 70, 80, 90, 100])\n",
        "precio = np.array([150000, 180000, 210000, 240000, 270000, 300000])\n",
        "\n",
        "plt.scatter(tamaño, precio)\n",
        "plt.xlabel('Tamaño (m²)')\n",
        "plt.ylabel('Precio (€)')\n",
        "plt.title('Relación tamaño-precio de viviendas')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "En este artículo no solo entenderás la teoría, sino que implementarás la regresión lineal desde cero y verás cómo aplicarla en proyectos reales de Data Science. Empezaremos con los fundamentos matemáticos, pasando por la implementación práctica, hasta las consideraciones avanzadas que todo data scientist debe conocer.\n",
        "\n",
        "**¿Preparado para dominar uno de los algoritmos más versátiles del machine learning?** Vamos allá."
      ],
      "metadata": {
        "id": "N-arE5vUcMZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n",
        "\n",
        "La regresión lineal es, sin duda, uno de los primeros algoritmos que cualquier *data scientist* aprende —y con razón. A pesar de su aparente simplicidad, sigue siendo una herramienta poderosa, interpretable y sorprendentemente útil en una amplia gama de problemas del mundo real: desde predecir ventas hasta estimar precios de viviendas o modelar tendencias en series temporales.\n",
        "\n",
        "### ¿Por qué mínimos cuadrados?\n",
        "\n",
        "En esencia, la regresión lineal busca modelar la relación entre una variable dependiente \\( y \\) y una o más variables independientes \\( X \\) mediante una línea (o hiperplano) recto. Pero, ¿cómo decidimos cuál es la “mejor” línea? Aquí entra el **método de los mínimos cuadrados**: minimiza la suma de los errores al cuadrado entre los valores observados y los predichos. Esta estrategia no solo tiene una solución analítica elegante, sino que también sienta las bases para entender técnicas más avanzadas como la regularización o el descenso del gradiente.\n",
        "\n",
        "En este artículo, veremos cómo implementar regresión lineal desde cero en Python, compararla con `scikit-learn`, evaluar su rendimiento y discutir cuándo —y cuándo no— usarla en tus proyectos de Data Science."
      ],
      "metadata": {
        "id": "Ho1Wj7pscSDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n",
        "\n",
        "La regresión lineal es, sin duda, uno de los algoritmos más fundamentales en Data Science. Aunque pueda parecer simple comparado con las redes neuronales o los ensemble methods de hoy en día, entender su funcionamiento es esencial para cualquier profesional del área.\n",
        "\n",
        "**¿Por qué es tan importante?** Porque establece las bases de cómo modelamos relaciones entre variables y cómo optimizamos funciones de coste. Además, muchos algoritmos modernos son extensiones o variaciones de estos conceptos básicos.\n",
        "\n",
        "El **método de mínimos cuadrados** (Ordinary Least Squares, OLS) resuelve un problema aparentemente sencillo: dada una nube de puntos, queremos encontrar la línea (o hiperplano en dimensiones superiores) que mejor se ajuste a los datos. \"Mejor\" aquí significa minimizar la suma de los errores cuadráticos entre las predicciones y los valores reales.\n",
        "\n",
        "Matemáticamente, buscamos los parámetros $\\boldsymbol{\\theta}$ que minimizan:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "donde $h_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\boldsymbol{\\theta}^T \\mathbf{x}$ es nuestra hipótesis lineal.\n",
        "\n",
        "En este artículo implementaremos este método desde cero en Python, compararemos con scikit-learn, y reflexionaremos sobre cuándo usar mínimos cuadrados frente a métodos iterativos como el descenso del gradiente."
      ],
      "metadata": {
        "id": "-quQFPhAcXBZ"
      }
    }
  ]
}