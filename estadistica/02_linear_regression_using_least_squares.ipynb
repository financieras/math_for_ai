{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNne93EAajIv05tJ2gshwuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/estadistica/02_linear_regression_using_least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Using Least Squares\n",
        "\n",
        "Este artículo forma parte de una serie titulada \"Linear Regression for Data Science\".  \n",
        "Se compone de tres artículos siendo este el primero:\n",
        "1. Linear Regression Using Least Squares (← este artículo)\n",
        "2. Linear Regression Using the Gradient Descent Algorithm\n",
        "3. Logarithmic, Exponential, and Polynomial Linear Regression"
      ],
      "metadata": {
        "id": "hB8P_OHO0_Pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introducción**\n",
        "\n",
        "En Machine Learning, se aconseja comenzar programando un algoritmo de regresión lineal, ya que además de formativo se continúa usando a nivel empresarial en Data Science. Pero ¿por qué sigue siendo tan relevante en la era de los chat bots y las redes neuronales?\n",
        "\n",
        "La respuesta es simple: **efectividad y interpretabilidad**. Mientras que modelos más complejos pueden actuar como \"cajas negras\", la regresión lineal nos permite entender exactamente cómo cada variable afecta a nuestro resultado. Es nuestro punto de partida natural para cualquier problema de predicción numérica.\n",
        "\n",
        "## ¿Qué problema resuelve realmente?\n",
        "\n",
        "Imagina que tienes datos históricos de precios de viviendas y quieres predecir cuánto costará una nueva casa. Tienes variables como metros cuadrados, número de habitaciones, ubicación... La regresión lineal te permite encontrar una relación matemática entre estas características y el precio."
      ],
      "metadata": {
        "id": "N-arE5vUcMZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos de ejemplo: tamaño de casa vs precio\n",
        "superficie = np.array([50, 55, 60, 64, 70, 78, 80, 89, 90, 100])\n",
        "precio = np.array([140000, 155000, 190000, 200000, 225000, 212000 ,240000, 230000 ,270000, 300000])\n",
        "\n",
        "plt.scatter(superficie, precio/1000, alpha=0.6, s=100)\n",
        "plt.xlabel('Superficie (m²)', fontsize=11)\n",
        "plt.ylabel('Precio (miles €)', fontsize=11)\n",
        "plt.title('Relación tamaño-precio de viviendas', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tL9kBiqHiEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El **método de mínimos cuadrados** (Ordinary Least Squares, OLS) resuelve un problema aparentemente sencillo: dada una nube de puntos, queremos encontrar la recta (o hiperplano) que mejor se ajuste a los datos. \"Mejor\" aquí significa minimizar la suma de los errores cuadráticos entre las predicciones y los valores reales."
      ],
      "metadata": {
        "id": "_bArB6FAjtIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Planteamiento general del problema de Regresión**\n",
        "\n",
        "Ahora que entendemos por qué la regresión lineal es tan útil, vamos a formalizar matemáticamente el problema. Comenzaremos con el caso más simple de una sola variable ($x$) para construir la intuición antes de generalizar al caso multivariable.\n",
        "\n",
        "## Caso simple: una variable independiente\n",
        "\n",
        "Empecemos con lo básico. Cuando tenemos solo una variable predictora $x$, queremos encontrar la recta que mejor se ajuste a nuestros datos:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lcl}\n",
        "y = w_0 + w_1 x + e         & \\longrightarrow & \\text{modelo real con error} \\\\\n",
        "\\hat{y} = w_0 + w_1 x       & \\longrightarrow & \\text{modelo estimado}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- $\\hat{y}$ → el **valor predicho** por el modelo, es la $y$ estimada (precio)\n",
        "- $x$ → variable predictora (superficie)\n",
        "- $w_0$ → **intercepto** (término independiente o bias)\n",
        "- $w_1$ → **pendiente** (coeficiente de la variable)\n",
        "- $e$ → **error o residuo**: la diferencia entre el valor real y lo que predice el modelo\n",
        "\n",
        "El error de cada predicción es:\n",
        "\n",
        "\\begin{align*}\n",
        "e_i &= y_i - \\hat{y}_i \\\\\n",
        "e_i &= y_i - (w_0 + w_1 x_i)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Nuestro objetivo es encontrar $w_0$ y $w_1$ que minimicen estos errores.\n",
        "\n",
        "## El método de Mínimos Cuadrados\n",
        "\n",
        "En lugar de usar los errores directamente trabajamos con los **errores al cuadrado**. Definimos la función de coste:\n",
        "\n",
        "$$J(w_0, w_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)]^2$$\n",
        "\n",
        "**¿Por qué al cuadrado?** Por tres razones principales:\n",
        "- Evita que errores positivos y negativos se cancelen\n",
        "- Penaliza más los errores grandes (lo que suele ser deseable)\n",
        "- Obtenemos una función diferenciable y convexa, lo que facilita encontrar el mínimo\n",
        "\n",
        "### El Error Cuadrático Medio (MSE)\n",
        "\n",
        "La función $J$ simplemente suma los errores al cuadrado, pero si calculamos la media habremos obtenido el Error Cuadrático Medio (MSE).\n",
        "\n",
        "$$\\text{MSE}(w) = \\frac{1}{n} \\sum_{i=1}^{n}e_i^2 = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (w_0 + w_1 x_i))^2$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Veámoslo gráficamente con nuestros datos de viviendas:"
      ],
      "metadata": {
        "id": "g4Dkeoddf4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de cómo varía el error con diferentes valores de w1\n",
        "def calcular_error(w0, w1, x, y):\n",
        "    y_pred = w0 + w1 * x\n",
        "    return np.sum((y - y_pred)**2)\n",
        "\n",
        "# Calculamos el error para diferentes valores de w1\n",
        "w1_vals = np.linspace(1000, 4000, 100)\n",
        "w0_fijo = 20_000  # Valor fijo para visualización\n",
        "errores = [calcular_error(w0_fijo, w1, superficie, precio) for w1 in w1_vals]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Gráfico del error vs w1\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w1_vals, errores, 'b-', linewidth=2)\n",
        "min_idx = np.argmin(errores)\n",
        "plt.plot(w1_vals[min_idx], errores[min_idx], 'ro', markersize=10, label='Mínimo')\n",
        "plt.xlabel('w₁ (pendiente)', fontsize=11)\n",
        "plt.ylabel('Error (suma de cuadrados)', fontsize=11)\n",
        "plt.title('Búsqueda del valor óptimo de w₁', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Gráfico con mejor ajuste\n",
        "plt.subplot(1, 2, 2)\n",
        "mejor_w1 = w1_vals[min_idx]\n",
        "y_pred = w0_fijo + mejor_w1 * superficie\n",
        "plt.scatter(superficie, precio/1000, alpha=0.6, s=100, label='Datos reales')\n",
        "plt.plot(superficie, y_pred/1000, 'r-', linewidth=2, label=f'Mejor ajuste (θ₁={mejor_w1:.0f})')\n",
        "plt.xlabel('Superficie (m²)', fontsize=11)\n",
        "plt.ylabel('Precio (miles €)', fontsize=11)\n",
        "plt.title('Ajuste con w₁ óptimo', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nw₁ óptimo encontrado: {mejor_w1:.2f}€/m²\")\n",
        "print(f\"Error mínimo (MSE): {errores[min_idx]:.2e}\")"
      ],
      "metadata": {
        "id": "visualizacion_error"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización de los errores"
      ],
      "metadata": {
        "id": "UVf-dWu-klfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(superficie, precio/1000, alpha=0.7, s=80, label='Datos observados')\n",
        "\n",
        "# Primero calculamos los coeficientes para la recta\n",
        "X = np.column_stack([np.ones(len(superficie)), superficie])\n",
        "w = np.linalg.inv(X.T @ X) @ X.T @ precio\n",
        "\n",
        "# Recta de regresión\n",
        "superficie_range = np.linspace(45, 105, 100)\n",
        "precio_range = w[0] + w[1] * superficie_range\n",
        "plt.plot(superficie_range, precio_range/1000, 'r-', linewidth=2, label='Recta de regresión')\n",
        "\n",
        "# Dibujar líneas de error (distancias verticales)\n",
        "for i in range(len(superficie)):\n",
        "    y_pred = w[0] + w[1] * superficie[i]\n",
        "    plt.plot([superficie[i], superficie[i]],\n",
        "             [precio[i]/1000, y_pred/1000],\n",
        "             'k--', alpha=0.5, linewidth=1)\n",
        "    # Puntos predichos\n",
        "    plt.plot(superficie[i], y_pred/1000, 'ro', markersize=4)\n",
        "\n",
        "plt.xlabel('Superficie (m²)')\n",
        "plt.ylabel('Precio (miles €)')\n",
        "plt.title('Visualización de los errores: distancias verticales al cuadrado')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F5gZKE_bgIsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada línea discontinua representa un error $e_i$. El método de mínimos cuadrados encuentra la recta que **minimiza la suma de los cuadrados de todas estas distancias verticales**.\n",
        "\n",
        "---\n",
        "### Minimizando el error\n",
        "\n",
        "El método encuentra los valores óptimos derivando e igualando a cero:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial w_0} = 0, \\quad \\frac{\\partial J}{\\partial w_1} = 0$$\n",
        "\n",
        "Esto nos da las **ecuaciones normales**, cuya solución es:\n",
        "\n",
        "$$w_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
        "\n",
        "$$w_0 = \\bar{y} - w_1 \\bar{x}$$\n",
        "\n",
        "Donde $\\bar{x}$ y $\\bar{y}$ son las medias de $x$ e $y$ respectivamente.\n",
        "\n",
        "### La fuerza de la relación lineal: el coeficiente de correlación $ r $\n",
        "\n",
        "Para evaluar nuestro modelo, es útil preguntarnos: **¿hasta qué punto están relacionadas linealmente $x$ e $y$?**\n",
        "\n",
        "El **coeficiente de correlación lineal $r$** mide precisamente eso: la **fuerza y dirección** de la relación lineal entre dos variables. Su valor está siempre entre $-1$ y $1$:\n",
        "- $r = 1$: relación lineal positiva perfecta (al aumentar $x$, $y$ aumenta proporcionalmente)\n",
        "- $r = -1$: relación lineal negativa perfecta\n",
        "- $r = 0$: no hay relación lineal\n",
        "- $|r|$ cercano a 1 → relación fuerte\n",
        "- $|r|$ cercano a 0 → relación débil\n",
        "\n",
        "En regresión lineal, el coeficiente de correlación lineal $r$ se define como el cociente entre la covarianza de las dos variables y el producto de sus desviaciones estándar. Matemáticamente,\n",
        "\n",
        "$$\n",
        "r = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
        "$$\n",
        "\n",
        "Desarrollando la fórmula obtenemos:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n",
        "$$\n",
        "\n",
        "> **Nota intuitiva**: El numerador mide cómo se mueven juntas las desviaciones de $x$ e $ y$ respecto a sus medias. Si siempre suben o bajan juntas, $r$ será alto. Si no, se cancelan y $r$ será bajo. Luego se divide entre el producto de las desviaciones para estandarizar entre -1 y 1.\n",
        "\n",
        "Este $r$ es especialmente útil **antes de ajustar el modelo**: si $|r|$ es muy bajo, ya sabemos que una recta no capturará bien la relación entre las variables.\n",
        "\n",
        "Estas fórmulas funcionan perfectamente para una variable independiente $X$, pero ¿qué pasa cuando tenemos múltiples características? Aquí es donde la notación matricial se vuelve indispensable.\n",
        "\n",
        "## Generalización a múltiples variables: notación matricial\n",
        "\n",
        "Cuando tenemos múltiples variables predictoras $(x_1, x_2, \\dots, x_n)$, el modelo se extiende naturalmente. En lugar de una recta, ahora buscamos un **hiperplano**:\n",
        "\n",
        "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$$\n",
        "\n",
        "Aquí es donde la **notación matricial** hace la vida más fácil. Representamos nuestros datos como:\n",
        "\n",
        "$$\\mathbf{X} = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_n\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{y} = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "**¡Ojo con la columna de unos!** Es crucial incluirla en $\\mathbf{X}$ para representar el término independiente $w_0$.\n",
        "\n",
        "Nuestro modelo se escribe elegantemente como:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}$$\n",
        "\n",
        "Y la solución óptima viene dada por las **ecuaciones normales** en forma matricial:\n",
        "\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
        "\n",
        "Esta fórmula nos da directamente los coeficientes óptimos sin necesidad de iterar. ¡Es como tener la respuesta de un problema de optimización sin tener que buscarla!"
      ],
      "metadata": {
        "id": "y1qtwEsvPHwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Por qué esta notación es tan poderosa?** Porque nos permite tratar casos simples y complejos con el mismo framework matemático, y porque las operaciones matriciales son computacionalmente eficientes en NumPy.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3jijSiAtiGGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Implementación con Mínimos Cuadrados**\n",
        "\n",
        "## De la teoría a la práctica\n",
        "\n",
        "Procedemos a implementar la solución de mínimos cuadrados desde cero, aplicando los conceptos teóricos desarrollados anteriormente a nuestro dataset de precios de viviendas.\n",
        "\n",
        "La formulación matricial que derivamos:\n",
        "\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
        "\n",
        "Se implementa eficientemente utilizando operaciones matriciales en NumPy."
      ],
      "metadata": {
        "id": "rqmy2c9uk5uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construcción de la matriz de diseño\n",
        "\n",
        "La implementación correcta requiere incluir una columna de unos para el término independiente:"
      ],
      "metadata": {
        "id": "SW_IZ8aVlMZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos los datos de precio y superficie del ejemplo inicial\n",
        "\n",
        "# Matriz X con columna de unos para el intercept\n",
        "X = np.column_stack([np.ones(len(superficie)), superficie])\n",
        "y = precio\n",
        "\n",
        "print(\"Matriz de diseño X:\")\n",
        "print(X)\n",
        "print(f\"\\nDimensiones: {X.shape} - {X.shape[0]} observaciones, {X.shape[1]} características\")"
      ],
      "metadata": {
        "id": "oBCGs-HklQR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación del algoritmo\n",
        "Ahora que entendemos la teoría, implementémos en código la expresión matricial."
      ],
      "metadata": {
        "id": "24x8dPVRlbiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimos_cuadrados(X, y):\n",
        "    \"\"\"Calcula los coeficientes óptimos usando mínimos cuadrados ordinarios\"\"\"\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# Cálculo de coeficientes\n",
        "w = minimos_cuadrados(X, y)\n",
        "\n",
        "print(\"Coeficientes del modelo:\")\n",
        "print(f\"w_0 (intercept): {w[0]:.2f} €\")\n",
        "print(f\"w_1 (pendiente): {w[1]:.2f} €/m²\")"
      ],
      "metadata": {
        "id": "nU-x24mWlf3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretación de coeficientes:**\n",
        "- **Pendiente (2,791.81 €/m²)**: Por cada metro cuadrado adicional, el precio aumenta aproximadamente 2,792 €\n",
        "- **Intercept (10,722.85 €)**: Representa el valor base de la vivienda (cuando la superficie es 0 m²)\n",
        "\n",
        "## Visualización del ajuste"
      ],
      "metadata": {
        "id": "6DbrZsz9lojP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones del modelo\n",
        "precio_predicho = X @ w\n",
        "\n",
        "# Visualización\n",
        "plt.scatter(superficie, precio/1000, alpha=0.7, s=80, label='Datos observados')\n",
        "\n",
        "# Recta de regresión\n",
        "superficie_range = np.linspace(45, 105, 100)\n",
        "precio_range = w[0] + w[1] * superficie_range\n",
        "plt.plot(superficie_range, precio_range/1000, 'r-', linewidth=2,\n",
        "         label='Recta de regresión')\n",
        "\n",
        "plt.xlabel('Superficie (m²)')\n",
        "plt.ylabel('Precio (miles €)')\n",
        "plt.title('Ajuste por Mínimos Cuadrados: Predicción vs Realidad')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dPLF1RAgl_SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "Para evaluar qué tan bien funciona nuestro modelo, utilizamos dos métricas fundamentales:\n",
        "\n",
        "**RMSE (Root Mean Square Error)**: Es la raíz cuadrada del MSE y nos dice el error promedio en las mismas unidades que nuestra variable objetivo (euros). Un RMSE bajo indica predicciones más precisas.\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "**R² (Coeficiente de Determinación)**: Indica qué proporción de la variabilidad de los datos es explicada por nuestro modelo. Varía entre 0 y 1, donde 1 significa ajuste perfecto.\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
        "\n",
        "### Conexión con $R^2$: del \"cuánto se mueven juntos\" al \"cuánto explica el modelo\"\n",
        "\n",
        "Ahora viene la magia: **en regresión lineal simple (una sola variable), $R^2 = r^2$**.\n",
        "\n",
        "Es decir:\n",
        "- $R^2$ responde: *“¿Qué porcentaje de la variabilidad en $y$ explica mi modelo?”*\n",
        "- $r^2$ responde lo mismo, **pero partiendo solo de la correlación**.\n",
        "\n",
        "Por eso:\n",
        "- Si $r = 0.5$, entonces $R^2 = 0.25$ → el modelo explica el **25%** de la variabilidad.\n",
        "- Si $r = 0$, entonces $R^2 = 0$ → el modelo no explica nada.\n",
        "\n",
        "> **En resumen**:  \n",
        "> - Usa $r$ para entender la **relación cruda** entre $x$ e $y$ y ver el signo para saber si la relación es directa o inversa.  \n",
        "> - Usa $R^2$ para saber **cuánto de esa relación captura tu modelo lineal**, y expresar esa relación en porcentaje."
      ],
      "metadata": {
        "id": "dIv_SiLlmEGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos el error cuadrático medio (MSE) y RMSE\n",
        "mse = np.mean((precio - precio_predicho) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Coeficiente de determinación R²\n",
        "ss_res = np.sum((precio - precio_predicho) ** 2)  # Suma de cuadrados de residuos\n",
        "ss_tot = np.sum((precio - precio.mean()) ** 2)    # Suma total de cuadrados\n",
        "r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Métricas de rendimiento\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MÉTRICAS DE RENDIMIENTO\")\n",
        "print(\"=\"*50)\n",
        "print(f\"R² (coeficiente de determinación): {r2:.3f}\")\n",
        "print(f\"  → El modelo explica el {r2*100:.1f}% de la variabilidad\")\n",
        "print(f\"\\nRMSE (error cuadrático medio):    {rmse:,.2f}€\")\n",
        "print(f\"  → Error promedio de {rmse:,.0f} € en las predicciones\")"
      ],
      "metadata": {
        "id": "GZqDjZoKmH-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un $R^2$ de 0.9 indica que nuestro modelo es excelente, explicando el 90% de la variabilidad en los precios. Sin embargo, el RMSE de ~14.5k€ nos recuerda que aún hay margen de error considerable en las predicciones individuales.\n",
        "\n",
        "### Interpretación\n",
        "- Un MSE más bajo es mejor, pero cuidado: depende de la escala de tus datos.\n",
        "- El coeficiente de determinación $R^2$ es la métrica más intuitiva:\n",
        "    - $0 \\le R^2 \\le 1$: El modelo explica un porcentaje de la variabilidad\n",
        "    - $R^2 = 1$: Modelo perfecto (explica el 100% de la variabilidad)\n",
        "    - $R^2 = 0.9$: El modelo explica el 90% de la variabilidad en los datos\n",
        "    - $R^2 = 0$: El modelo no es mejor que simplemente predecir el promedio\n",
        "    - $R^2 < 0$: IMPOSIBLE matemáticamente\n",
        "\n",
        "## Haciendo predicciones"
      ],
      "metadata": {
        "id": "LKGlBcxOd0dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones para nuevas casas\n",
        "nuevas_superficies = np.array([65, 85, 95])\n",
        "predicciones = w[0] + w[1] * nuevas_superficies\n",
        "\n",
        "print(\"\\nPREDICCIONES PARA NUEVAS VIVIENDAS\")\n",
        "print(\"-\" * 34)\n",
        "for sup, pred in zip(nuevas_superficies, predicciones):\n",
        "    print(f\"   Casa de {sup}m²  →  {pred:,.0f}€\")"
      ],
      "metadata": {
        "id": "T0nwWjqq0edR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparación con scikit-learn"
      ],
      "metadata": {
        "id": "E99vM8Zld5Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Creamos y entrenamos el modelo\n",
        "modelo_sklearn = LinearRegression()\n",
        "modelo_sklearn.fit(superficie.reshape(-1, 1), precio)\n",
        "\n",
        "# Comparamos resultados\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARACIÓN: Nuestra Implementación vs scikit-learn\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n{'Parámetro':<25} {'Nuestra impl.':<20} {'scikit-learn':<20}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'w₀ (intercepto)':<25} {w[0]:>15,.2f}€   {modelo_sklearn.intercept_:>15,.2f}€\")\n",
        "print(f\"{'w₁ (pendiente)':<25} {w[1]:>15,.2f}€/m² {modelo_sklearn.coef_[0]:>14,.2f}€/m²\")\n",
        "print(\"\\n¡Los resultados son idénticos! ✓\")"
      ],
      "metadata": {
        "id": "oYTFVW4W0gEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Ventajas, Limitaciones y Cuándo Usarlo**\n",
        "\n",
        "## Ventajas del método de mínimos cuadrados\n",
        "\n",
        "- **Solución exacta**: Encuentra los coeficientes óptimos directamente, sin iteraciones\n",
        "- **Rápido y eficiente**: Para datasets pequeños y medianos, es computacionalmente muy eficiente\n",
        "- **Interpretabilidad**: Los coeficientes tienen una interpretación directa y clara\n",
        "- **Garantía matemática**: Si existe solución, este método la encuentra\n",
        "- **Sin hiperparámetros**: No requiere ajuste de learning rate u otros parámetros\n",
        "\n",
        "## Limitaciones importantes"
      ],
      "metadata": {
        "id": "MyFfz5WWeunw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de problema con multicolinealidad\n",
        "X_problema = np.column_stack([superficie, superficie * 2])  # Columnas linealmente dependientes\n",
        "print(\"Matriz con columnas linealmente dependientes:\")\n",
        "print(X_problema[:3])\n",
        "print(f\"\\nDeterminante de X.T @ X: {np.linalg.det(X_problema.T @ X_problema):.10f}\")\n",
        "print(\"Un determinante cercano a 0 indica problemas de inversión matricial\")"
      ],
      "metadata": {
        "id": "MxawDmVwevly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problemas comunes:**\n",
        "- **Matrices singulares**: Cuando las variables son linealmente dependientes\n",
        "- **Multicolinealidad**: Variables predictoras muy correlacionadas entre sí\n",
        "- **No escala bien**: Para datasets muy grandes (millones de registros), la inversión de matriz se vuelve costosa ($O(n^3)$)\n",
        "- **Sensibilidad a outliers**: Los errores al cuadrado amplifican el efecto de valores atípicos\n",
        "\n",
        "Ahora que conocemos las limitaciones del método, surge la pregunta natural: ¿cuándo debemos usar mínimos cuadrados y cuándo recurrir a alternativas como Gradient Descent?\n",
        "\n",
        "## ¿Cuándo usar mínimos cuadrados vs Gradient Descent?\n",
        "\n",
        "**Usa mínimos cuadrados cuando:**\n",
        "- Tienes < 10,000 muestras y < 100 características\n",
        "- Necesitas la solución exacta en una sola operación\n",
        "- El dataset cabe cómodamente en memoria RAM\n",
        "\n",
        "**Usa Gradient Descent cuando:**\n",
        "- Tienes millones de registros o cientos de características\n",
        "- El dataset no cabe en memoria (puedes usar mini-batches)\n",
        "- Necesitas actualizar el modelo con nuevos datos continuamente\n",
        "- Trabajas con redes neuronales u otros modelos no lineales\n",
        "\n",
        "---\n",
        "\n",
        "# **5. Conclusión**\n",
        "\n",
        "Hemos visto cómo el método de mínimos cuadrados nos permite encontrar la mejor recta que se ajusta a nuestros datos mediante una solución matemática elegante y directa.\n",
        "\n",
        "**En resumen:**\n",
        "- **Fácil de implementar** con pocas líneas de código\n",
        "- **Resultados interpretables** que podemos explicar a cualquier stakeholder\n",
        "- **Extremadamente efectivo** para problemas con relaciones lineales\n",
        "- **Base fundamental** para entender métodos más complejos\n",
        "\n",
        "**Próximos pasos:** En el siguiente artículo de esta serie, exploraremos el **algoritmo de Gradient Descent**, que nos permitirá escalar a problemas más grandes, allanando el camino hacia técnicas más avanzadas de Machine Learning."
      ],
      "metadata": {
        "id": "h1GOTQ9wPTkl"
      }
    }
  ]
}