{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/estadistica/articulo1_regresion_lineal_por_minimos_cuadrados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-lJKSnnpF5g"
      },
      "source": [
        "# Linear Regression from Scratch with Python: OLS Explained\n",
        "Regresión Lineal desde cero con Python: Mínimos Cuadrados paso a paso\n",
        "\n",
        "La regresión lineal es el algoritmo fundamental en Data Science, y en este artículo aprenderás a construirlo desde cero. Combinamos teoría accesible con código práctico para que entiendas **por qué funciona** y **cómo implementarlo** paso a paso.\n",
        "\n",
        "Al final de este artículo, tendrás un modelo de regresión lineal funcionando y visualizado, listo para hacer predicciones.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introducción**\n",
        "\n",
        "En Machine Learning, se aconseja comenzar programando un algoritmo de regresión lineal, ya que además de formativo se continúa usando a nivel empresarial en Data Science. Pero ¿por qué sigue siendo tan relevante en la era de los chatbots y las redes neuronales?\n",
        "\n",
        "La respuesta es simple: **efectividad y interpretabilidad**. Mientras que modelos más complejos pueden actuar como \"cajas negras\", la regresión lineal nos permite entender exactamente cómo cada variable afecta a nuestro resultado. Es nuestro punto de partida natural para cualquier problema de predicción numérica.\n",
        "\n",
        "## ¿Qué problema resuelve realmente?\n",
        "\n",
        "Imagina que tienes datos históricos de precios de viviendas y quieres predecir cuánto costará una nueva casa. Tienes variables como metros cuadrados, número de habitaciones, ubicación... La regresión lineal te permite encontrar una relación matemática entre estas características y el precio."
      ],
      "metadata": {
        "id": "N-arE5vUcMZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos de ejemplo: tamaño de casa vs precio\n",
        "superficie = np.array([50, 55, 60, 64, 70, 78, 80, 89, 90, 100])\n",
        "precio = np.array([140000, 155000, 190000, 200000, 225000, 212000 ,240000, 230000 ,270000, 300000])\n",
        "\n",
        "plt.scatter(superficie, precio/1000, alpha=0.6, s=100)\n",
        "plt.xlabel('Superficie (m²)', fontsize=11)\n",
        "plt.ylabel('Precio (miles €)', fontsize=11)\n",
        "plt.title('Relación tamaño-precio de viviendas', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tL9kBiqHiEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El **método de mínimos cuadrados** (Ordinary Least Squares, OLS) resuelve un problema aparentemente sencillo: dada una nube de puntos, queremos encontrar la recta (o hiperplano) que mejor se ajuste a los datos. \"Mejor\" aquí significa minimizar la suma de los errores cuadráticos entre las predicciones y los valores reales."
      ],
      "metadata": {
        "id": "_bArB6FAjtIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Planteamiento general del problema de Regresión**\n",
        "\n",
        "Ahora que entendemos por qué la regresión lineal es tan útil, vamos a formalizar matemáticamente el problema. Comenzaremos con el caso más simple de una sola variable ($x$) para construir la intuición antes de generalizar al caso multivariable.\n",
        "\n",
        "## Caso simple: una variable independiente\n",
        "\n",
        "Empecemos con lo básico. Cuando tenemos solo una variable predictora $x$, queremos encontrar la recta que mejor se ajuste a nuestros datos:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lcl}\n",
        "y = w_0 + w_1 x + e         & \\longrightarrow & \\text{modelo real con error} \\\\\n",
        "\\hat{y} = w_0 + w_1 x       & \\longrightarrow & \\text{modelo estimado}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- $\\hat{y}$ → el **valor predicho** por el modelo, es la $y$ estimada (precio)\n",
        "- $x$ → variable predictora (superficie)\n",
        "- $w_0$ → **intercepto** (término independiente o bias)\n",
        "- $w_1$ → **pendiente** (coeficiente de la variable)\n",
        "- $e$ → **error o residuo**: la diferencia entre el valor real y lo que predice el modelo\n",
        "\n",
        "El error de cada predicción es:\n",
        "\n",
        "\\begin{align*}\n",
        "e_i &= y_i - \\hat{y}_i \\\\\n",
        "e_i &= y_i - (w_0 + w_1 x_i)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Nuestro objetivo es encontrar $w_0$ y $w_1$ que minimicen estos errores.\n",
        "\n",
        "## El método de Mínimos Cuadrados\n",
        "\n",
        "En lugar de usar los errores directamente trabajamos con los **errores al cuadrado**. Definimos la función de coste:\n",
        "\n",
        "$$J(w_0, w_1) = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} [y_i - (w_0 + w_1 x_i)]^2$$\n",
        "\n",
        "**¿Por qué al cuadrado?** Por tres razones principales:\n",
        "- Evita que errores positivos y negativos se cancelen\n",
        "- Penaliza más los errores grandes (lo que suele ser deseable)\n",
        "- Obtenemos una función diferenciable y convexa, lo que facilita encontrar el mínimo\n",
        "\n",
        "### El Error Cuadrático Medio (MSE)\n",
        "\n",
        "La función $J(w)$ simplemente suma los errores al cuadrado, pero si calculamos la media habremos obtenido el Error Cuadrático Medio (MSE).\n",
        "\n",
        "$$\\text{MSE}(w) = \\frac{1}{n} \\sum_{i=1}^{n}e_i^2 = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - (w_0 + w_1 x_i))^2$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Veámoslo gráficamente con nuestros datos de viviendas:"
      ],
      "metadata": {
        "id": "g4Dkeoddf4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de cómo varía el error con diferentes valores de w1\n",
        "def calcular_error(w0, w1, x, y):\n",
        "    y_pred = w0 + w1 * x\n",
        "    return np.sum((y - y_pred)**2)\n",
        "\n",
        "# Calculamos el error para diferentes valores de w1\n",
        "w1_vals = np.linspace(1000, 4000, 100)\n",
        "w0_fijo = 20_000  # Valor fijado arbitrariamene para visualización\n",
        "errores = [calcular_error(w0_fijo, w1, superficie, precio) for w1 in w1_vals]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Gráfico del error vs w1\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w1_vals, errores, 'b-', linewidth=2)\n",
        "min_idx = np.argmin(errores)\n",
        "plt.plot(w1_vals[min_idx], errores[min_idx], 'ro', markersize=10, label='Mínimo')\n",
        "plt.xlabel('w₁ (pendiente)', fontsize=11)\n",
        "plt.ylabel('Error (suma de cuadrados)', fontsize=11)\n",
        "plt.title('Búsqueda del valor óptimo de w₁', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Gráfico con mejor ajuste\n",
        "plt.subplot(1, 2, 2)\n",
        "mejor_w1 = w1_vals[min_idx]\n",
        "y_pred = w0_fijo + mejor_w1 * superficie\n",
        "plt.scatter(superficie, precio/1000, alpha=0.6, s=100, label='Datos reales')\n",
        "plt.plot(superficie, y_pred/1000, 'r-', linewidth=2, label=f'Mejor ajuste (θ₁={mejor_w1:.0f})')\n",
        "plt.xlabel('Superficie (m²)', fontsize=11)\n",
        "plt.ylabel('Precio (miles €)', fontsize=11)\n",
        "plt.title('Ajuste con w₁ óptimo', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nw₁ óptimo encontrado: {mejor_w1:.2f}€/m²\")\n",
        "print(f\"Error mínimo (MSE): {errores[min_idx]:.2e}\")"
      ],
      "metadata": {
        "id": "visualizacion_error"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización de los errores"
      ],
      "metadata": {
        "id": "UVf-dWu-klfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(superficie, precio/1000, alpha=0.7, s=80, label='Datos observados')\n",
        "\n",
        "# Primero calculamos los coeficientes para la recta\n",
        "X = np.column_stack([np.ones(len(superficie)), superficie])  # agregamos columna de 1s para bias\n",
        "w = np.linalg.inv(X.T @ X) @ X.T @ precio\n",
        "\n",
        "# Recta de regresión\n",
        "superficie_range = np.linspace(45, 105, 100)\n",
        "precio_range = w[0] + w[1] * superficie_range\n",
        "plt.plot(superficie_range, precio_range/1000, 'r-', linewidth=2, label='Recta de regresión')\n",
        "\n",
        "# Dibujar líneas de error (distancias verticales)\n",
        "for i in range(len(superficie)):\n",
        "    y_pred = w[0] + w[1] * superficie[i]\n",
        "    plt.plot([superficie[i], superficie[i]],\n",
        "             [precio[i]/1000, y_pred/1000],\n",
        "             'k--', alpha=0.5, linewidth=1)\n",
        "    # Puntos predichos\n",
        "    plt.plot(superficie[i], y_pred/1000, 'ro', markersize=4)\n",
        "\n",
        "plt.xlabel('Superficie (m²)')\n",
        "plt.ylabel('Precio (miles €)')\n",
        "plt.title('Visualización de los errores: distancias verticales al cuadrado')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F5gZKE_bgIsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada línea discontinua representa un error $e_i$. El método de mínimos cuadrados encuentra la recta que **minimiza la suma de los cuadrados de todas estas distancias verticales**.\n",
        "\n",
        "---\n",
        "### Minimizando el error\n",
        "\n",
        "El método encuentra los valores óptimos derivando e igualando a cero:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial w_0} = 0, \\quad \\frac{\\partial J}{\\partial w_1} = 0$$\n",
        "\n",
        "Esto nos da las **ecuaciones normales**, cuya solución es:\n",
        "\n",
        "$$w_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
        "\n",
        "$$w_0 = \\bar{y} - w_1 \\bar{x}$$\n",
        "\n",
        "Donde $\\bar{x}$ y $\\bar{y}$ son las medias de $x$ e $y$ respectivamente.\n",
        "\n",
        "### La fuerza de la relación lineal: el coeficiente de correlación $ r $\n",
        "\n",
        "Para evaluar nuestro modelo, es útil preguntarnos: **¿hasta qué punto están relacionadas linealmente $x$ e $y$?**\n",
        "\n",
        "El **coeficiente de correlación lineal $r$** mide precisamente eso: la **fuerza y dirección** de la relación lineal entre dos variables. Su valor está siempre entre $-1$ y $1$:\n",
        "- $r = 1$: relación lineal positiva perfecta (al aumentar $x$, $y$ aumenta proporcionalmente)\n",
        "- $r = -1$: relación lineal negativa perfecta\n",
        "- $r = 0$: no hay relación lineal\n",
        "- $|r|$ cercano a 1 → relación fuerte\n",
        "- $|r|$ cercano a 0 → relación débil\n",
        "\n",
        "En regresión lineal, el coeficiente de correlación lineal $r$ se define como el cociente entre la covarianza de las dos variables y el producto de sus desviaciones estándar. La covarianza mide cómo varían juntas las dos variables.\n",
        "\n",
        "Matemáticamente,\n",
        "\n",
        "$$\n",
        "r = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
        "$$\n",
        "\n",
        "Desarrollando la fórmula obtenemos:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n",
        "$$\n",
        "\n",
        "> **Nota intuitiva**: El numerador mide cómo se mueven juntas las desviaciones de $x$ e $ y$ respecto a sus medias. Si siempre suben o bajan juntas, $r$ será alto. Si no, se cancelan y $r$ será bajo. Luego se divide entre el producto de las desviaciones para estandarizar entre -1 y 1.\n",
        "\n",
        "Este $r$ es especialmente útil **antes de ajustar el modelo**: si $|r|$ es muy bajo, ya sabemos que una recta no capturará bien la relación entre las variables.\n",
        "\n",
        "Estas fórmulas funcionan perfectamente para una variable independiente $X$, pero ¿qué pasa cuando tenemos múltiples características? Aquí es donde la notación matricial se vuelve indispensable.\n",
        "\n",
        "## Generalización a múltiples variables: notación matricial\n",
        "\n",
        "Cuando tenemos múltiples variables predictoras $(x_1, x_2, \\dots, x_n)$, el modelo se extiende naturalmente. En lugar de una recta, ahora buscamos un **hiperplano**:\n",
        "\n",
        "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$$\n",
        "\n",
        "Aquí es donde la **notación matricial** hace la vida más fácil. Representamos nuestros datos como:\n",
        "\n",
        "$$\\mathbf{X} = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_n\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{y} = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "**¡Ojo con la columna de unos!** Es crucial incluirla en $\\mathbf{X}$ para representar el término independiente $w_0$.\n",
        "\n",
        "Nuestro modelo se escribe elegantemente como:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}$$\n",
        "\n",
        "Y la solución óptima viene dada por las **ecuaciones normales** en forma matricial:\n",
        "\n",
        "$$\\boxed{\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}}$$\n",
        "\n",
        "Esta fórmula nos da directamente los coeficientes óptimos sin necesidad de iterar. ¡Es como tener la respuesta de un problema de optimización sin tener que buscarla!"
      ],
      "metadata": {
        "id": "y1qtwEsvPHwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Por qué esta notación es tan poderosa?** Porque nos permite tratar casos simples y complejos con el mismo framework matemático, y porque las operaciones matriciales son computacionalmente eficientes en NumPy.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3jijSiAtiGGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Implementación con Mínimos Cuadrados**\n",
        "\n",
        "## De la teoría a la práctica\n",
        "\n",
        "Procedemos a implementar la solución de mínimos cuadrados desde cero, aplicando los conceptos teóricos desarrollados anteriormente a nuestro dataset de precios de viviendas.\n",
        "\n",
        "La formulación matricial que derivamos:\n",
        "\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
        "\n",
        "Se implementa eficientemente utilizando operaciones matriciales en NumPy."
      ],
      "metadata": {
        "id": "rqmy2c9uk5uX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construcción de la matriz de diseño\n",
        "\n",
        "La implementación correcta requiere incluir una columna de unos para el término independiente:"
      ],
      "metadata": {
        "id": "SW_IZ8aVlMZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos los datos de precio y superficie del ejemplo inicial\n",
        "\n",
        "# Matriz X con columna de unos para el intercept\n",
        "X = np.column_stack([np.ones(len(superficie)), superficie])\n",
        "y = precio\n",
        "\n",
        "print(\"Matriz de diseño X:\")\n",
        "print(X)\n",
        "print(f\"\\nDimensiones: {X.shape} - {X.shape[0]} observaciones, {X.shape[1]} características\")"
      ],
      "metadata": {
        "id": "oBCGs-HklQR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ddfcc1-af09-4062-f11a-fe6cb06845df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de diseño X:\n",
            "[[  1.  50.]\n",
            " [  1.  55.]\n",
            " [  1.  60.]\n",
            " [  1.  64.]\n",
            " [  1.  70.]\n",
            " [  1.  78.]\n",
            " [  1.  80.]\n",
            " [  1.  89.]\n",
            " [  1.  90.]\n",
            " [  1. 100.]]\n",
            "\n",
            "Dimensiones: (10, 2) - 10 observaciones, 2 características\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación del algoritmo\n",
        "Ahora que entendemos la teoría, implementémos en código la expresión matricial."
      ],
      "metadata": {
        "id": "24x8dPVRlbiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimos_cuadrados(X, y):\n",
        "    \"\"\"Calcula los coeficientes óptimos usando mínimos cuadrados ordinarios\"\"\"\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# Cálculo de coeficientes\n",
        "w = minimos_cuadrados(X, y)\n",
        "\n",
        "print(\"Coeficientes del modelo:\")\n",
        "print(f\"w_0 (intercept): {w[0]:.2f} €\")\n",
        "print(f\"w_1 (pendiente): {w[1]:.2f} €/m²\")"
      ],
      "metadata": {
        "id": "nU-x24mWlf3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1c4a01-bc99-4e0d-faea-86ae61c50c7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coeficientes del modelo:\n",
            "w_0 (intercept): 10722.85 €\n",
            "w_1 (pendiente): 2791.81 €/m²\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretación de coeficientes:**\n",
        "- **Pendiente (2,791.81 €/m²)**: Por cada metro cuadrado adicional, el precio aumenta aproximadamente 2,792 €\n",
        "- **Intercept (10,722.85 €)**: Representa el valor base de la vivienda (cuando la superficie es 0 m²)\n",
        "\n",
        "## Visualización del ajuste"
      ],
      "metadata": {
        "id": "6DbrZsz9lojP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones del modelo\n",
        "precio_predicho = X @ w\n",
        "\n",
        "# Visualización\n",
        "plt.scatter(superficie, precio/1000, alpha=0.7, s=80, label='Datos observados')\n",
        "\n",
        "# Recta de regresión\n",
        "superficie_range = np.linspace(45, 105, 100)\n",
        "precio_range = w[0] + w[1] * superficie_range\n",
        "plt.plot(superficie_range, precio_range/1000, 'r-', linewidth=2,\n",
        "         label='Recta de regresión')\n",
        "\n",
        "plt.xlabel('Superficie (m²)')\n",
        "plt.ylabel('Precio (miles €)')\n",
        "plt.title('Ajuste por Mínimos Cuadrados: Predicción vs Realidad')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dPLF1RAgl_SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-SFlI0MpF5l"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Resumen\n",
        "\n",
        "Hemos implementado con éxito un modelo de regresión lineal desde cero usando el método de mínimos cuadrados. Ahora tienes:\n",
        "\n",
        "- Un modelo que predice precios basándose en la superficie\n",
        "- Visualización clara del ajuste a los datos\n",
        "- Comprensión de la formulación matricial\n",
        "\n",
        "**¿Y ahora qué?** Nuestro modelo hace predicciones, pero ¿cómo sabes si son buenas? ¿Cuándo deberías usar este método y cuándo no? ¿Cómo se compara con implementaciones profesionales?\n",
        "\n",
        "Estas preguntas críticas se responden en el siguiente artículo, donde evaluaremos el rendimiento del modelo y exploraremos sus ventajas, limitaciones y casos de uso apropiados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Etiqutas para el blog\n",
        "\n",
        "```txt\n",
        "#Python\n",
        "#DataScience\n",
        "#MachineLearning\n",
        "#LinearRegression\n",
        "#OLS\n",
        "#NumPy\n",
        "#DataAnalysis\n",
        "#Statistics\n",
        "#Coding\n",
        "#Tutorial\n",
        "#DataVisualization\n",
        "#BeginnerFriendly\n",
        "#RegressionModeling\n",
        "```"
      ],
      "metadata": {
        "id": "hQlH8pNL_YNV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}