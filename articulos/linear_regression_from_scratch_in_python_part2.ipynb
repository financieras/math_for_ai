{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMGv1A2dviFAJ0rdSSqYab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/articulos/linear_regression_from_scratch_in_python_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression from Scratch in Python. Part 2/2**\n",
        "\n",
        "#### **Evaluation, Metrics & Real-World Use**\n",
        "\n",
        "You already know how to implement linear regression using least squares. But a model that makes predictions isn't useful if you don't know **how good those predictions are** and **when it's appropriate to use it**.\n",
        "\n",
        "In this article, you'll learn to evaluate your model, interpret its results, and decide when to use this method. By the end, you'll understand the key metrics, the method's limitations, and when to consider alternatives.\n",
        "\n",
        "**Note**: This article continues where we left off in the previous article. If you haven't implemented linear regression using least squares yet, start there to get the most out of this content.\n",
        "\n",
        "---\n",
        "\n",
        "# **1. Initial Setup**\n",
        "\n",
        "First, let's recreate the model we built in the previous article using the same data:"
      ],
      "metadata": {
        "id": "Ubv1LwQVwXEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data: housing size vs price (same as previous article)\n",
        "area = np.array([50, 55, 60, 64, 70, 78, 80, 89, 90, 100])\n",
        "price = np.array([140000, 155000, 190000, 200000, 225000,\n",
        "                  212000, 240000, 230000, 270000, 300000])\n",
        "\n",
        "# Design matrix\n",
        "X = np.column_stack([np.ones(len(area)), area])\n",
        "y = price\n",
        "\n",
        "def least_squares(X, y):\n",
        "    \"\"\"Calculates optimal coefficients using least squares\"\"\"\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# Calculate coefficients and predictions\n",
        "w = least_squares(X, y)\n",
        "y_pred = X @ w\n",
        "\n",
        "print(f\"Calculated coefficients:\")\n",
        "print(f\"  w₀ (intercept) = ${w[0]:,.2f}\")\n",
        "print(f\"  w₁ (slope) = ${w[1]:,.2f}/m²\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0c4XrNtwbpc",
        "outputId": "176de97c-fc64-47bf-a888-26be774986df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated coefficients:\n",
            "  w₀ (intercept) = $10,722.85\n",
            "  w₁ (slope) = $2,791.81/m²\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **2. Evaluation Metrics**\n",
        "\n",
        "Now that we have predictions, we need to quantify how good they are. We'll use three fundamental metrics that give us complementary perspectives on the model's performance.\n",
        "\n",
        "## Correlation coefficient (r): Measuring the linear relationship\n",
        "\n",
        "Before evaluating our model, we need to understand a fundamental metric: the **Pearson linear correlation coefficient (r)**, which measures the strength and direction of the linear relationship between two variables.\n",
        "\n",
        "**Values of r:**\n",
        "- r = 1: perfect positive linear relationship (when x increases, y increases proportionally)\n",
        "- r = -1: perfect negative linear relationship (when x increases, y decreases proportionally)  \n",
        "- r = 0: no linear relationship\n",
        "- |r| > 0.7: strong linear relationship\n",
        "- |r| < 0.3: weak linear relationship\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$r = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{m} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{m} (y_i - \\bar{y})^2}}$$\n",
        "\n",
        "Or equivalently using covariance and standard deviations:\n",
        "\n",
        "$$r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
        "\n",
        "> **Intuitive interpretation**: The numerator measures how the deviations of x and y from their means move together. If both variables increase or decrease together, r will be close to 1 or -1. If there's no pattern, r will be near 0.\n",
        "\n",
        "**Why is it important to calculate it?**  \n",
        "The coefficient r tells us **before training the model** whether there's a linear relationship worth modeling. If |r| is very low, we know in advance that linear regression won't be effective.\n",
        "\n",
        "## R²: Explained variability\n",
        "\n",
        "**R² (Coefficient of Determination)** indicates what proportion of the data's variability our model explains. It ranges between 0 and 1, where 1 means perfect fit.\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{m}(y_i - \\bar{y})^2}$$\n",
        "\n",
        "**Practical interpretation:**\n",
        "- **R² = 1**: Perfect model (explains 100% of variability)\n",
        "- **R² = 0.95**: The model explains 95% of the variability in the data\n",
        "- **R² = 0**: The model is no better than simply predicting the mean\n",
        "\n",
        "## Connection between r and R²\n",
        "\n",
        "Here's an important mathematical detail: **in simple linear regression (one variable), R² = r²**.\n",
        "\n",
        "This means the coefficient of determination is simply the square of the correlation coefficient.\n",
        "\n",
        "**Practical implications:**\n",
        "\n",
        "| Correlation (r) | R² | Interpretation |\n",
        "|-----------------|----|----------------|\n",
        "| r = 0.95 | R² = 0.90 | The model explains 90% of variability |\n",
        "| r = 0.70 | R² = 0.49 | The model explains 49% of variability |\n",
        "| r = 0.50 | R² = 0.25 | The model explains only 25% of variability |\n",
        "| r = -0.90 | R² = 0.81 | The model explains 81% (sign doesn't affect R²) |\n",
        "\n",
        "**Key differences:**\n",
        "- **r** tells you the direction (with the ± sign) and the strength of the relationship\n",
        "- **R²** is always positive and tells you what percentage of variability the model explains\n",
        "- In multiple regression (several variables), only R² exists, there's no single r\n",
        "\n",
        "## RMSE: The average error\n",
        "\n",
        "**RMSE (Root Mean Square Error)** measures the typical magnitude of the prediction errors in the same units as the target variable (dollars). For example, an RMSE of \\\\$15,000 means predictions are typically off by around \\\\$15,000 — larger errors are more heavily penalized because they are squared in the calculation.\n",
        "\n",
        "$$\\text{RMSE} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "**Why is it useful?**\n",
        "- Unlike R², RMSE is in the **same units** as your target variable (dollars, meters, etc.)\n",
        "- It gives a concrete sense of \"how far off\" the predictions are on average\n",
        "- Because errors are squared before averaging, larger errors are penalized more heavily\n",
        "- In practice, RMSE approximates the **typical error magnitude** — e.g., with RMSE = \\\\$14,574, predictions are typically off by around \\\\$14,000–\\\\$15,000\n",
        "- It's easy to communicate: \"On average, our model is off by about \\\\$15,000.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Calculating the metrics\n"
      ],
      "metadata": {
        "id": "razGz6S6w-AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "\n",
        "# Correlation coefficient r (for simple regression)\n",
        "r = np.corrcoef(area, price)[0, 1]\n",
        "\n",
        "# MSE and RMSE\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Coefficient of determination R²\n",
        "ss_res = np.sum((y - y_pred) ** 2)  # Residual sum of squares\n",
        "ss_tot = np.sum((y - y.mean()) ** 2)  # Total sum of squares\n",
        "r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"r (correlation): {r:.4f}\")\n",
        "print(f\"  → {'Strong' if abs(r) > 0.7 else 'Moderate' if abs(r) > 0.4 else 'Weak'} linear relationship\")\n",
        "print(f\"\\nR² (coef. of determination): {r2:.4f}\")\n",
        "print(f\"  → The model explains {r2*100:.2f}% of variability\")\n",
        "print(f\"\\nVerification: r² = {r**2:.4f} ≈ R² = {r2:.4f} ✓\")\n",
        "print(f\"\\nRMSE (root mean square error): ${rmse:,.2f}\")\n",
        "print(f\"  → Average error of approximately ${rmse:,.0f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcbksEEyxKkx",
        "outputId": "77ec0ce5-3f8d-4ffd-addd-f5b3893e5f99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PERFORMANCE METRICS\n",
            "==================================================\n",
            "r (correlation): 0.9488\n",
            "  → Strong linear relationship\n",
            "\n",
            "R² (coef. of determination): 0.9001\n",
            "  → The model explains 90.01% of variability\n",
            "\n",
            "Verification: r² = 0.9001 ≈ R² = 0.9001 ✓\n",
            "\n",
            "RMSE (root mean square error): $14,573.71\n",
            "  → Average error of approximately $14,574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Visualizing the Fit and Making Predictions**\n",
        "\n",
        "## Visualizing the fit\n",
        "\n",
        "The metrics give us numbers, but visualizing the fit helps us better understand the model's performance:"
      ],
      "metadata": {
        "id": "vAmm43FlxXT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model visualization\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subplot 1: Model fit\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(area, price/1000, alpha=0.6, s=100, label='Actual data')\n",
        "plt.plot(area, y_pred/1000, 'r-', linewidth=2, label='Linear fit')\n",
        "plt.xlabel('Area (m²)', fontsize=11)\n",
        "plt.ylabel('Price (thousands $)', fontsize=11)\n",
        "plt.title(f'Model Fit (R² = {r2:.3f})', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Subplot 2: Residual analysis\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y - y_pred\n",
        "plt.scatter(y_pred/1000, residuals/1000, alpha=0.6, s=100)\n",
        "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Predicted Price (thousands $)', fontsize=11)\n",
        "plt.ylabel('Residuals (thousands $)', fontsize=11)\n",
        "plt.title(f'Residual Analysis (RMSE = ${rmse/1000:.1f}k)', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nResidual analysis shows how far each prediction is from the actual value.\")\n",
        "print(\"Ideally, residuals should be randomly distributed around zero.\")"
      ],
      "metadata": {
        "id": "T6Uth1wSyMue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions\n",
        "\n",
        "With our model evaluated, we can use it to predict prices for new properties:"
      ],
      "metadata": {
        "id": "SBiKM-F1yWUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for new properties\n",
        "new_areas = np.array([65, 85, 95])\n",
        "predictions = w[0] + w[1] * new_areas\n",
        "\n",
        "print(\"\\nPREDICTIONS FOR NEW PROPERTIES\")\n",
        "print(\"-\" * 40)\n",
        "for area_val, pred in zip(new_areas, predictions):\n",
        "    print(f\"   {area_val}m² house  →  ${pred:,.0f}\")\n",
        "\n",
        "print(f\"\\n(Remember: predictions have ~${rmse:,.0f} average error)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa89Rh2DyZKD",
        "outputId": "586f5db6-56c8-4c8b-f438-26621e5eaa91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PREDICTIONS FOR NEW PROPERTIES\n",
            "----------------------------------------\n",
            "   65m² house  →  $192,190\n",
            "   85m² house  →  $248,027\n",
            "   95m² house  →  $275,945\n",
            "\n",
            "(Remember: predictions have ~$14,574 average error)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **4. Comparison with scikit-learn**\n",
        "\n",
        "An excellent way to validate our implementation is to compare it with the industry-standard library:"
      ],
      "metadata": {
        "id": "Y12uM61Vys4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create and train the scikit-learn model\n",
        "sklearn_model = LinearRegression()\n",
        "sklearn_model.fit(area.reshape(-1, 1), price)\n",
        "\n",
        "# Predictions with scikit-learn\n",
        "sklearn_pred = sklearn_model.predict(area.reshape(-1, 1))\n",
        "sklearn_rmse = np.sqrt(mean_squared_error(price, sklearn_pred))\n",
        "sklearn_r2 = r2_score(price, sklearn_pred)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(\"COMPARISON: Our Implementation vs scikit-learn\")\n",
        "print(\"=\"*65)\n",
        "print(f\"\\n{'Parameter':<20} {'Our impl.':<22} {'scikit-learn':<22}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'w₀ (intercept)':<20} ${w[0]:>18,.2f}   ${sklearn_model.intercept_:>18,.2f}\")\n",
        "print(f\"{'w₁ (slope)':<20} ${w[1]:>17,.2f}/m²  ${sklearn_model.coef_[0]:>17,.2f}/m²\")\n",
        "print(f\"\\n{'R²':<20} {r2:>21.4f}   {sklearn_r2:>21.4f}\")\n",
        "print(f\"{'RMSE':<20} ${rmse:>20,.2f}   ${sklearn_rmse:>20,.2f}\")\n",
        "print(\"\\n✓ Results are identical! Our implementation is correct.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICqQ1D00zAhH",
        "outputId": "a949bdd5-3484-432c-c487-7e532599910f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=================================================================\n",
            "COMPARISON: Our Implementation vs scikit-learn\n",
            "=================================================================\n",
            "\n",
            "Parameter            Our impl.              scikit-learn          \n",
            "-----------------------------------------------------------------\n",
            "w₀ (intercept)       $         10,722.85   $         10,722.85\n",
            "w₁ (slope)           $         2,791.81/m²  $         2,791.81/m²\n",
            "\n",
            "R²                                  0.9001                  0.9001\n",
            "RMSE                 $           14,573.71   $           14,573.71\n",
            "\n",
            "✓ Results are identical! Our implementation is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **5. Advantages, Limitations, and When to Use It**\n",
        "\n",
        "Now that we understand how it works and how to evaluate it, it's crucial to know when it's appropriate to use this method.\n",
        "\n",
        "## Advantages of the least squares method\n",
        "\n",
        "The Ordinary Least Squares (OLS) method has characteristics that make it especially valuable:\n",
        "\n",
        "- **Exact solution**: Finds optimal coefficients directly, without iterations or approximations\n",
        "- **Speed**: For small and medium datasets, it's computationally very efficient\n",
        "- **Interpretability**: Coefficients have a direct and clear interpretation you can explain to stakeholders\n",
        "- **Mathematical guarantee**: If a solution exists, this method finds it\n",
        "- **No hyperparameters**: Doesn't require tuning learning rate or other parameters\n",
        "\n",
        "## Important limitations\n",
        "\n",
        "However, the method has important limitations you should be aware of:"
      ],
      "metadata": {
        "id": "zKnucsIRzNYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of multicollinearity problem\n",
        "X_problem = np.column_stack([area, area * 2])  # Linearly dependent columns\n",
        "\n",
        "print(\"Example with linearly dependent columns:\")\n",
        "print(\"\\nFirst 3 rows of problematic matrix:\")\n",
        "print(X_problem[:3])\n",
        "print(f\"\\nDeterminant of X.T @ X: {np.linalg.det(X_problem.T @ X_problem):.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33OndfZozOiY",
        "outputId": "d7f4e195-5a55-48f7-b05b-49105a70c1fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example with linearly dependent columns:\n",
            "\n",
            "First 3 rows of problematic matrix:\n",
            "[[ 50 100]\n",
            " [ 55 110]\n",
            " [ 60 120]]\n",
            "\n",
            "Determinant of X.T @ X: 0.00e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main problems with the method:**\n",
        "\n",
        "1. **Singular matrices / Perfect collinearity**  \n",
        "   When predictor variables are linearly dependent (one is an exact linear combination of others), the matrix $(X^T X)$ becomes singular and cannot be inverted.  \n",
        "   In the example above, the second column is exactly twice the first → the determinant of $X^T X$ is exactly 0 → no unique least squares solution exists.\n",
        "\n",
        "2. **Multicollinearity (high but not perfect correlation)**  \n",
        "   Even when features are not perfectly collinear, strong correlations between predictors make the estimated coefficients unstable and hard to interpret (small changes in the data can cause large swings in the coefficients).\n",
        "\n",
        "3. **Limited scalability**  \n",
        "   For very large datasets (millions of records) or high-dimensional data, matrix inversion has O(n³) complexity and becomes prohibitively expensive.\n",
        "\n",
        "4. **Sensitivity to outliers**  \n",
        "   Because errors are squared, extreme values have a disproportionately large influence and can significantly bias the model.\n",
        "\n",
        "5. **Memory consumption**  \n",
        "   Requires loading the entire dataset into RAM.\n",
        "\n",
        "**Practical solutions for multicollinearity:**\n",
        "- Remove one of the highly correlated features\n",
        "- Combine correlated features (e.g., using Principal Component Analysis – PCA)\n",
        "- Switch to regularized regression methods (Ridge or Lasso), which are more robust to collinearity\n",
        "\n",
        "## When to use Least Squares vs Gradient Descent?\n",
        "\n",
        "The choice between these methods depends mainly on the size and nature of your data:\n",
        "\n",
        "**Use least squares when:**\n",
        "- You have few features (typically fewer than a few hundred on consumer hardware)\n",
        "- You need the exact analytical solution in a single operation\n",
        "- The dataset fits comfortably in RAM\n",
        "- Interpretability of individual coefficients is important\n",
        "- There are no severe multicollinearity issues\n",
        "\n",
        "**Use Gradient Descent when:**\n",
        "- You have millions of records or hundreds/thousands of features\n",
        "- The dataset doesn't fit in memory (you can use mini-batches)\n",
        "- You need to update the model continuously with new data (online learning)\n",
        "- You're working with neural networks or other non-linear models\n",
        "- You want built-in regularization (L1, L2) for better handling of collinearity or feature selection\n",
        "\n",
        "**The reason to choose**: Beyond several hundred features, the cost and numerical instability of matrix inversion grow rapidly, making iterative methods like gradient descent far more practical for large-scale or high-dimensional problems.\n",
        "\n",
        "**Practical scalability rule:**\n",
        "- < 10,000 observations and < 100 features → Least squares (fast and exact)\n",
        "- \\> 100,000 observations or > 1,000 features → Gradient Descent (scalable)\n",
        "- Between 10k–100k → Both work; choose based on your specific needs\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d4Rnn8uMzY4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Conclusion**\n",
        "\n",
        "In this article, we completed our journey through linear regression with least squares, learning not only to build the model but also to evaluate it correctly and understand when to use it.\n",
        "\n",
        "## Recap\n",
        "\n",
        "**What we learned:**\n",
        "- How to measure performance with **RMSE** and **R²**\n",
        "- The relationship between R² and correlation\n",
        "- How to visualize and validate the model fit\n",
        "- The advantages and limitations of the method\n",
        "- When to choose least squares vs Gradient Descent\n",
        "\n",
        "**Why it matters:**\n",
        "Linear regression using least squares is:\n",
        "- **Your starting point** for any numerical prediction problem\n",
        "- **Extremely effective** when there are linear relationships in the data\n",
        "- **Interpretable and explainable**, crucial in business contexts\n",
        "- **The foundation** for understanding more advanced methods\n",
        "\n",
        "## A practical note for beginners\n",
        "\n",
        "Here's some straightforward advice as you start your journey in Data Science and Machine Learning:\n",
        "\n",
        "1. **For real-world projects: Reach for scikit-learn first**  \n",
        "   In almost every practical scenario, your go-to choice should be scikit-learn's `LinearRegression`. It handles the implementation details reliably, works well regardless of dataset size, and is numerically robust — saving you from worrying about the underlying solver.\n",
        "\n",
        "2. **For hands-on learning and future-proof skills: Implement gradient descent**  \n",
        "   When building models from scratch to deepen your understanding, prioritize **gradient descent** over the closed-form least squares solution. Gradient descent scales to problems of any size and forms the foundation of nearly all modern machine learning optimization, especially in deep learning. Iterative methods like this are the tools you'll use throughout your career.\n",
        "\n",
        "3. **For building intuition: Study the least squares (normal equations) method**  \n",
        "   The normal equations are excellent for developing intuition about convex optimization and understanding what \"best-fitting line\" really means in linear regression. However, most real-world loss functions are not convex, so treat least squares primarily as a powerful teaching tool — in practice, rely on libraries or gradient descent.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "Now that you've mastered the closed-form solution, the natural next step is to explore the **Gradient Descent algorithm**. This method will allow you to:\n",
        "- Scale to problems with millions of data points\n",
        "- Understand iterative optimization\n",
        "- Prepare for Deep Learning\n",
        "\n",
        "In the next article in this series, we'll implement Gradient Descent from scratch and compare its performance with least squares, giving you all the tools to decide which method to use in each situation.\n",
        "\n",
        "---\n",
        "\n",
        "> ### Thank you\n",
        "\n",
        "---\n",
        "© 2025\n",
        "---"
      ],
      "metadata": {
        "id": "IcOhR6KozZ3w"
      }
    }
  ]
}