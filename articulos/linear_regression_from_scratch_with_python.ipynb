{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/financieras/math_for_ai/blob/main/articulos/linear_regression_from_scratch_with_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-lJKSnnpF5g"
      },
      "source": [
        "# **Linear Regression from Scratch in Python (Part 1/2)**\n",
        "**OLS Derivation & Implementation**\n",
        "\n",
        "Linear regression is the fundamental algorithm in Data Science, and in this article you'll learn how to build it from scratch. We combine accessible theory with practical code so you understand **why it works** and **how to implement it** step by step.\n",
        "\n",
        "By the end of this article, you'll have a working and visualized linear regression model, ready to make predictions.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**\n",
        "\n",
        "In Machine Learning, it's recommended to start by programming a linear regression algorithm, since besides being educational, it continues to be used at the enterprise level in Data Science. But why does it remain so relevant in the age of chatbots and neural networks?\n",
        "\n",
        "The answer is simple: **effectiveness and interpretability**. While more complex models can act as \"black boxes,\" linear regression allows us to understand exactly how each variable affects our outcome. It's our natural starting point for any numerical prediction problem.\n",
        "\n",
        "## What problem does it really solve?\n",
        "\n",
        "Imagine you have historical data on housing prices and want to predict how much a new home will cost. You have variables like square meters, number of rooms, location... Linear regression lets you find a mathematical relationship between these features and the price."
      ],
      "metadata": {
        "id": "N-arE5vUcMZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data: housing size vs price\n",
        "area = np.array([50, 55, 60, 64, 70, 78, 80, 89, 90, 100])\n",
        "price = np.array([140000, 155000, 190000, 200000, 225000,\n",
        "                  212000, 240000, 230000, 270000, 300000])\n",
        "\n",
        "plt.scatter(area, price/1000, alpha=0.6, s=100)\n",
        "plt.xlabel('Area (m²)', fontsize=11)\n",
        "plt.ylabel('Price (thousands $)', fontsize=11)\n",
        "plt.title('Size-Price Relationship of Housing', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tL9kBiqHiEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **least squares method** (Ordinary Least Squares, OLS) solves a seemingly simple problem: given a cloud of points, we want to find the line (or hyperplane) that best fits the data. \"Best\" here means minimizing the sum of squared errors between predictions and actual values."
      ],
      "metadata": {
        "id": "_bArB6FAjtIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. General Formulation of the Regression Problem**\n",
        "\n",
        "Now that we understand why linear regression is so useful, let's formalize the problem mathematically. We'll start with the simplest case of a single variable ($x$) to build intuition before generalizing to the multivariate case.\n",
        "\n",
        "**Note on Notation.** Our conventions:\n",
        "- we use **$m$** to denote the total number of training examples (observations)\n",
        "- while **$n$** represents the number of features (independent variables)\n",
        "\n",
        "\n",
        "## Simple case: one independent variable\n",
        "\n",
        "Let's start with the basics. When we have only one predictor variable $x$, we want to find the line that best fits our data:\n",
        "\n",
        "$$\n",
        "\\begin{array}{lcl}\n",
        "y = w_0 + w_1 x + e         & \\longrightarrow & \\text{true model with error} \\\\\n",
        "\\hat{y} = w_0 + w_1 x       & \\longrightarrow & \\text{estimated model}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{y}$ → the **predicted value** by the model, the estimated $y$ (price)\n",
        "- $x$ → predictor variable (area)\n",
        "- $w_0$ → **intercept** (independent term or bias)\n",
        "- $w_1$ → **slope** (coefficient of the variable)\n",
        "- $e$ → **error or residual**: the difference between the actual value and what the model predicts\n",
        "\n",
        "The error of each prediction is:\n",
        "\n",
        "\\begin{align*}\n",
        "e_i &= y_i - \\hat{y}_i \\\\\n",
        "e_i &= y_i - (w_0 + w_1 x_i)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Our goal is to find $w_0$ and $w_1$ that minimize these errors.\n",
        "\n",
        "## The Least Squares Method\n",
        "\n",
        "Instead of using the errors directly, we work with **squared errors**. We define the cost function:\n",
        "\n",
        "$$J(w_0, w_1) = \\sum_{i=1}^{m} e_i^2 = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{m} [y_i - (w_0 + w_1 x_i)]^2$$\n",
        "\n",
        "**Why squared?** For three main reasons:\n",
        "- Prevents positive and negative errors from canceling out\n",
        "- Penalizes large errors more (which is usually desirable)\n",
        "- We obtain a differentiable and convex function, which makes finding the minimum easier\n",
        "\n",
        "### The Mean Squared Error (MSE)\n",
        "\n",
        "The function $J(w)$ simply sums the squared errors, but if we calculate the mean we'll have obtained the Mean Squared Error (MSE).\n",
        "\n",
        "$$\\text{MSE}(w) = \\frac{1}{m} \\sum_{i=1}^{m}e_i^2 = \\frac{1}{m} \\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2 = \\frac{1}{m} \\sum_{i=1}^{m}(y_i - (w_0 + w_1 x_i))^2$$"
      ],
      "metadata": {
        "id": "g4Dkeoddf4-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of Errors"
      ],
      "metadata": {
        "id": "UVf-dWu-klfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(area, price/1000, alpha=0.7, s=80, label='Observed data')\n",
        "\n",
        "# First we calculate the coefficients for the line\n",
        "X = np.column_stack([np.ones(len(area)), area])  # add column of 1s for bias\n",
        "\n",
        "# Apply formula: w = (X^T X)^-1 X^T y\n",
        "w = np.linalg.inv(X.T @ X) @ X.T @ price\n",
        "\n",
        "\n",
        "# Regression line\n",
        "area_range = np.linspace(45, 105, 100)\n",
        "price_range = w[0] + w[1] * area_range\n",
        "plt.plot(area_range, price_range/1000, 'r-', linewidth=2, label='Regression line')\n",
        "\n",
        "# Vertical lines showing the errors\n",
        "for xi, yi in zip(area, price):\n",
        "    yi_pred = w[0] + w[1] * xi\n",
        "    plt.plot([xi, xi], [yi/1000, yi_pred/1000], 'g--', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Area (m²)', fontsize=11)\n",
        "plt.ylabel('Price (thousands $)', fontsize=11)\n",
        "plt.title('Residuals: Distance from Each Point to the Line', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNLA65PMkoZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The green dashed lines represent the **residuals** or errors: the vertical distance from each point to the regression line. The least squares method finds the line that makes the sum of the squares of these distances (green lines) as small as possible."
      ],
      "metadata": {
        "id": "YlIOvvgh8Vbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimizing the error\n",
        "\n",
        "The method finds the optimal values by taking derivatives and setting them equal to zero:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial w_0} = 0 \\quad \\text{and} \\quad \\frac{\\partial J}{\\partial w_1} = 0$$\n",
        "\n",
        "Solving this system of equations yields the **normal equations**:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "w_1 &= \\frac{m\\sum x_i y_i - \\sum x_i \\sum y_i}{m\\sum x_i^2 - (\\sum x_i)^2} \\\\[10pt]\n",
        "w_0 &= \\bar{y} - w_1 \\bar{x}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Where $\\bar{x}$ and $\\bar{y}$ are the means of $x$ and $y$ respectively."
      ],
      "metadata": {
        "id": "LWUOg8F18Zvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The strength of the linear relationship: the correlation coefficient $r$\n",
        "\n",
        "To evaluate our model, it's useful to ask ourselves: **to what extent are $x$ and $y$ linearly related?**\n",
        "\n",
        "The **linear correlation coefficient $r$** measures precisely that: the **strength and direction** of the linear relationship between two variables. Its value is always between $-1$ and $1$:\n",
        "- $r = 1$: perfect positive linear relationship (as $x$ increases, $y$ increases proportionally)\n",
        "- $r = -1$: perfect negative linear relationship\n",
        "- $r = 0$: no linear relationship\n",
        "- $|r|$ close to 1 → strong relationship\n",
        "- $|r|$ close to 0 → weak relationship\n",
        "\n",
        "In linear regression, the linear correlation coefficient $r$ is defined as the quotient between the covariance of the two variables and the product of their standard deviations. Covariance measures how the two variables vary together.\n",
        "\n",
        "Mathematically,\n",
        "\n",
        "$$\n",
        "r = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
        "$$\n",
        "\n",
        "Expanding the formula we obtain:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{m} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{m} (y_i - \\bar{y})^2}}\n",
        "$$\n",
        "\n",
        "> **Intuitive note**: The numerator measures how the deviations of $x$ and $y$ from their means move together. If they always go up or down together, $r$ will be high. If not, they cancel out and $r$ will be low. Then it's divided by the product of the deviations to standardize between -1 and 1.\n",
        "\n",
        "This $r$ is especially useful **before fitting the model**: if $|r|$ is very low, we already know that a straight line won't capture the relationship between the variables well.\n",
        "\n",
        "These formulas work perfectly for one independent variable $X$, but what happens when we have multiple features? This is where matrix notation becomes indispensable."
      ],
      "metadata": {
        "id": "cViLDmm4-MQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization to multiple variables: matrix notation\n",
        "\n",
        "When we have multiple predictor variables $(x_1, x_2, \\dots, x_n)$, the model extends naturally. Instead of a line, we now seek a **hyperplane**:\n",
        "\n",
        "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$$\n",
        "\n",
        "This is where **matrix notation** makes life easier. We represent our data as:\n",
        "\n",
        "$$\\mathbf{X} = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_n\n",
        "\\end{bmatrix}, \\quad\n",
        "\\mathbf{y} = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "**Watch out for the column of ones!** It's crucial to include it in $\\mathbf{X}$ to represent the independent term $w_0$.\n",
        "\n",
        "Our model is elegantly written as:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}$$\n",
        "\n",
        "And the optimal solution is given by the **normal equations** in matrix form:\n",
        "\n",
        "$$\\boxed{\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}}$$\n",
        "\n",
        "This formula gives us the optimal coefficients directly without needing to iterate. It's like having the answer to an optimization problem without having to search for it!\n",
        "\n",
        "**Why is this notation so powerful?** Because it allows us to treat simple and complex cases with the same mathematical framework, and because matrix operations are computationally efficient in NumPy.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ofpkEomH-2dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Implementation with Least Squares**\n",
        "\n",
        "## From theory to practice\n",
        "\n",
        "We proceed to implement the least squares solution from scratch, applying the theoretical concepts developed previously to our housing price dataset.\n",
        "\n",
        "The matrix formulation we derived:\n",
        "\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
        "\n",
        "Is efficiently implemented using matrix operations in NumPy.\n",
        "\n",
        "## Construction of the design matrix\n",
        "\n",
        "Proper implementation requires including a column of ones for the independent term:"
      ],
      "metadata": {
        "id": "Y3XgXW_zAUAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the price and area data from the initial example\n",
        "\n",
        "# Matrix X with column of ones for the intercept\n",
        "X = np.column_stack([np.ones(len(area)), area])\n",
        "y = price\n",
        "\n",
        "print(\"Design matrix X:\")\n",
        "print(X)\n",
        "print(f\"\\nDimensions: {X.shape} - {X.shape[0]} observations, {X.shape[1]} features\")"
      ],
      "metadata": {
        "id": "oMyKryW3AbcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1facd4ae-fc74-42ba-d08a-425a2ae98ce9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Design matrix X:\n",
            "[[  1.  50.]\n",
            " [  1.  55.]\n",
            " [  1.  60.]\n",
            " [  1.  64.]\n",
            " [  1.  70.]\n",
            " [  1.  78.]\n",
            " [  1.  80.]\n",
            " [  1.  89.]\n",
            " [  1.  90.]\n",
            " [  1. 100.]]\n",
            "\n",
            "Dimensions: (10, 2) - 10 observations, 2 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm implementation\n",
        "Now that we understand the theory, let's implement the matrix expression in code."
      ],
      "metadata": {
        "id": "yTiVWpSkA82x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ordinary_least_squares(X, y):\n",
        "    \"\"\"Calculates optimal coefficients using ordinary least squares\"\"\"\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "# Calculation of coefficients\n",
        "w = ordinary_least_squares(X, y)\n",
        "\n",
        "print(\"Model coefficients:\")\n",
        "print(f\"w_0 (intercept): ${w[0]:.2f}\")\n",
        "print(f\"w_1 (slope): ${w[1]:.2f} /m²\")"
      ],
      "metadata": {
        "id": "CbcmJ0nbA_19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a10204-3199-4f26-d604-e9adaf88af29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model coefficients:\n",
            "w_0 (intercept): $10722.85\n",
            "w_1 (slope): $2791.81 /m²\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coefficient interpretation:**\n",
        "- **Slope (\\$2,791.81 /m²)**: For each additional square meter, the price increases approximately \\$2,792\n",
        "- **Intercept ($10,722.85)**: Theoretical base price when area = 0 m² (not practically meaningful)"
      ],
      "metadata": {
        "id": "ezXCVmZBBAXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of the fit\n",
        "To clearly illustrate how the cost function behaves, we fix the intercept $w_0$ at its optimal value and show a one-dimensional slice of the error surface by varying only the slope $w_1$. This reveals the characteristic convex \"U\" shape with a single global minimum."
      ],
      "metadata": {
        "id": "cTxZeXjdBYpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see it graphically with our housing data:\n",
        "# Visualization of how error varies with different values of w1\n",
        "def calculate_error(w0, w1, x, y):\n",
        "    y_pred = w0 + w1 * x\n",
        "    return np.sum((y - y_pred)**2)\n",
        "\n",
        "# Calculate error for different values of w1\n",
        "w1_vals = np.linspace(1000, 4000, 100)\n",
        "w0_fixed = w[0]  # Once the optimal value of w_0 is known, we set it.\n",
        "errors = [calculate_error(w0_fixed, w1, area, price) for w1 in w1_vals]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Error vs w1 plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(w1_vals, errors, 'b-', linewidth=2)\n",
        "min_idx = np.argmin(errors)\n",
        "plt.plot(w1_vals[min_idx], errors[min_idx], 'ro', markersize=10, label='Minimum')\n",
        "plt.xlabel('w₁ (slope)', fontsize=11)\n",
        "plt.ylabel('Error (sum of squares)', fontsize=11)\n",
        "plt.title('Search for Optimal w₁ Value', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Plot with best fit\n",
        "plt.subplot(1, 2, 2)\n",
        "best_w1 = w1_vals[min_idx]\n",
        "y_pred = w0_fixed + best_w1 * area\n",
        "plt.scatter(area, price/1000, alpha=0.6, s=100, label='Actual data')\n",
        "plt.plot(area, y_pred/1000, 'r-', linewidth=2, label=f'Best fit (w₁={best_w1:.0f})')\n",
        "plt.xlabel('Area (m²)', fontsize=11)\n",
        "plt.ylabel('Price (thousands $)', fontsize=11)\n",
        "plt.title('Fit with Optimal w₁', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n- Optimal w₁ found: ${best_w1:.2f} /m²\")\n",
        "print(f\"- Minimum SSE (Sum of Squared Errors): {errors[min_idx]:.2e}\")\n",
        "print(f\"\\tWhich corresponds to MSE: {errors[min_idx]/len(area):.2e}\")"
      ],
      "metadata": {
        "id": "visualizacion_error"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Understanding the Graph**  \n",
        "> The plot on the left visualizes the **Cost Function**.\n",
        "- It shows how the Mean Squared Error (MSE) changes as we vary the slope ($w_1$).\n",
        "- Notice the \"U\" shape (convexity): this confirms there is a single global minimum.\n",
        "- The red dot represents the specific value calculated by our OLS formula, proving that the mathematical method successfully finds the exact bottom of the error valley without needing to test all possibilities.\n",
        "\n",
        "### Comparison of methods"
      ],
      "metadata": {
        "id": "hkPyVLiL-zo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nComparison of methods:\")\n",
        "print(f\"- Least squares (exact solution): ${w[1]:.2f} /m²\")\n",
        "print(f\"- Grid search (limited resolution): ${best_w1:.2f} /m²\")\n",
        "print(f\"- Difference: ${abs(w[1] - best_w1):.2f} /m² ({100*abs(w[1]-best_w1)/w[1]:.2f}%)\")\n",
        "print(f\"\\nThe small difference is due to:\")\n",
        "print(f\"1. The grid search uses only 100 values between 1000 and 4000\")\n",
        "print(f\"2. The real minimum (2791.81) falls between grid points\")\n",
        "print(f\"3. With more points or interpolation, they would match exactly\")"
      ],
      "metadata": {
        "id": "QSigTOF9-0Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2208bc04-359c-44a4-f872-cef437fadd61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of methods:\n",
            "- Least squares (exact solution): $2791.81 /m²\n",
            "- Grid search (limited resolution): $2787.88 /m²\n",
            "- Difference: $3.93 /m² (0.14%)\n",
            "\n",
            "The small difference is due to:\n",
            "1. The grid search uses only 100 values between 1000 and 4000\n",
            "2. The real minimum (2791.81) falls between grid points\n",
            "3. With more points or interpolation, they would match exactly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Regression Line\n",
        "We have successfully calculated the optimal coefficients ($\\mathbf{w_0}$ and $\\mathbf{w_1}$) using **our implementation** of the OLS method. The resulting equation is the **best-fit linear model** for the data. Let's visualize the final regression line against our dataset to see the model's performance."
      ],
      "metadata": {
        "id": "hf-ShTm7ANYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model predictions\n",
        "predicted_price = X @ w\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(area, price/1000, alpha=0.7, s=80, label='Observed data')\n",
        "\n",
        "# Regression line\n",
        "area_range = np.linspace(45, 105, 100)\n",
        "price_range = w[0] + w[1] * area_range\n",
        "plt.plot(area_range, price_range/1000, 'r-', linewidth=2,\n",
        "         label='Regression line')\n",
        "\n",
        "plt.xlabel('Area (m²)')\n",
        "plt.ylabel('Price (thousands $)')\n",
        "plt.title('Least Squares Fit: Prediction vs Reality')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6dFEfcU3BgnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "We have successfully implemented a linear regression model from scratch using the least squares method. You now have:\n",
        "\n",
        "- A model that predicts prices based on area\n",
        "- Clear visualization of the fit to the data\n",
        "- Understanding of the matrix formulation\n",
        "\n",
        "**What's next?** Our model makes predictions, but how do you know if they're good? When should you use this method and when not? How does it compare with professional implementations?\n",
        "\n",
        "These critical questions are answered in the next article, where we'll evaluate the model's performance and explore its advantages, limitations, and appropriate use cases."
      ],
      "metadata": {
        "id": "ltAt3M-uCAdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Blog Tags\n",
        "\n",
        "```txt\n",
        "#Python\n",
        "#DataScience\n",
        "#MachineLearning\n",
        "#LinearRegression\n",
        "#OLS\n",
        "#NumPy\n",
        "#DataAnalysis\n",
        "#Statistics\n",
        "#Coding\n",
        "#Tutorial\n",
        "#DataVisualization\n",
        "```"
      ],
      "metadata": {
        "id": "Q8oJLvNrEmA_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}